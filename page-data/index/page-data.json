{"componentChunkName":"component---src-pages-index-js","path":"/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","configs":{"countOfInitialPost":10}}},"allMarkdownRemark":{"edges":[{"node":{"excerpt":"ChatGPT와 후속 LLM(Large Language Model)이 등장하면서 “RLHF” 라고 불리는 “인간 피드백을 통한 강화학습”의 중요성에 대한 많은 논의가 있었다.   나는 “RL이 Supervised Learning보다 더 나은 이유가 무엇일까?”, “언어모델을 지침을 통해 학습하는것(Instruction fine-tuning)만으로 충분하지…","fields":{"slug":"/NLP/rlforlm/"},"frontmatter":{"date":"December 16, 2023","title":"Reinforcement Learning for Language Model","category":"NLP","draft":false}}},{"node":{"excerpt":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung…","fields":{"slug":"/NLP/chatgpt_beyond/"},"frontmatter":{"date":"April 16, 2023","title":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning","category":"NLP","draft":false}}},{"node":{"excerpt":"https://t.co/CIU4E5NLC8 LeCun은 AI 업계에서 가장 저명한 과학자 중 한 명이며 ChatGPT와 같은 AI 챗봇이 사용하는 기본 기술의 기능을 과장하는 사람들을 노골적으로 비판해 왔습니다. Barron’s: ChatGPT와 대규모 언어 모델(LLM)의 기술이 어떻게 작동하는지 설명해 주시겠습니까? LeCun: 초강력 예측 키보드라고…","fields":{"slug":"/NLP/lecun_interview/"},"frontmatter":{"date":"April 16, 2023","title":"AI 갓파더 얀 르쿤의 AI, LLM에 대한 인터뷰내용","category":"NLP","draft":false}}},{"node":{"excerpt":"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models\nVithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, Shreyas Saxena\nhttps:/…","fields":{"slug":"/NLP/spdf/"},"frontmatter":{"date":"April 16, 2023","title":"SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models","category":"NLP","draft":false}}},{"node":{"excerpt":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, S…","fields":{"slug":"/Multi_Modal/beit3/"},"frontmatter":{"date":"August 30, 2022","title":"[논문리뷰] Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","category":"Multi_Modal","draft":false}}},{"node":{"excerpt":"AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao https://arxiv.org/ab…","fields":{"slug":"/NLP/adamix/"},"frontmatter":{"date":"August 09, 2022","title":"[논문리뷰] AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models","category":"NLP","draft":false}}},{"node":{"excerpt":"LaMDA: Language Models for Dialog Applications Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang …","fields":{"slug":"/NLP/lamda/"},"frontmatter":{"date":"March 25, 2022","title":"[논문리뷰] LaMDA: Language Models for Dialog Applications","category":"NLP","draft":false}}},{"node":{"excerpt":"An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,…","fields":{"slug":"/vision/vit/"},"frontmatter":{"date":"October 13, 2020","title":"[논문리뷰] An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale","category":"Vision","draft":false}}},{"node":{"excerpt":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau …","fields":{"slug":"/NLP/rag/"},"frontmatter":{"date":"June 01, 2020","title":"[논문리뷰] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks","category":"NLP","draft":false}}},{"node":{"excerpt":"Integrated Eojeol Embeddings for Erroneous Sentence Classification in Korean Chatbots DongHyun Choi, IlNam Park, Myeong Cheol Shin, EungGyun Kim and Dong Ryeol Shin https://arxiv.org/abs/2004.05744 1…","fields":{"slug":"/NLP/iee/"},"frontmatter":{"date":"April 22, 2020","title":"[논문리뷰] Integrated Eojeol Embeddings for Erroneous Sentence Classification in Korean Chatbots","category":"NLP","draft":false}}},{"node":{"excerpt":"Pre-training Tasks for Embedding-based Large-scale Retrieval Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar https://openreview.net/pdf?id=rkg-mA4FDr 1. Introduction 본 논문에서는 la…","fields":{"slug":"/NLP/para_level_pretraining/"},"frontmatter":{"date":"March 29, 2020","title":"[논문리뷰] Pre-training Tasks for Embedding-based Large-scale Retrieval","category":"NLP","draft":false}}},{"node":{"excerpt":"Latent Retrieval for Weakly Supervised Open Domain Question Answering Kenton Lee, Ming-Wei Chang, Kristina Toutanova https://arxiv.org/abs/1906.00300 1. Introduction MRC의 발전으로 Open-domain question an…","fields":{"slug":"/NLP/orqa/"},"frontmatter":{"date":"March 03, 2020","title":"[논문리뷰] Latent Retrieval for Weakly Supervised Open Domain Question Answering","category":"NLP","draft":false}}},{"node":{"excerpt":"From English To Foreign Languages: Transferring Pre-trained Language Models Ke Tran https://arxiv.org/abs/2002.07306 Abstract Multilingual pre-trained model을 사용하면 NLP task를 resource가 많은 언어에서 resource…","fields":{"slug":"/NLP/ramen/"},"frontmatter":{"date":"February 25, 2020","title":"[논문리뷰] From English To Foreign Languages: Transferring Pre-trained Language Models","category":"NLP","draft":false}}},{"node":{"excerpt":"REALM: Retrieval-Augmented Language Model Pre-Training Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang https://kentonl.com/pub/gltpc.2020.pdf Abstract Language model pre-training은…","fields":{"slug":"/NLP/realm/"},"frontmatter":{"date":"February 20, 2020","title":"[논문리뷰] REALM: Retrieval-Augmented Language Model Pre-Training","category":"NLP","draft":false}}},{"node":{"excerpt":"How Much Knowledge Can You Pack Into the Parameters of a Language Model? Adam Roberts, Colin Raffel, Noam Shazeer https://colinraffel.com/publications/arxiv2020how.pdf Abstract 최근 비정형 텍스트로부터 학습된 lang…","fields":{"slug":"/NLP/knowledge_pack_lm/"},"frontmatter":{"date":"February 18, 2020","title":"[논문리뷰] How Much Knowledge Can You Pack Into the Parameters of a Language Model?","category":"NLP","draft":false}}},{"node":{"excerpt":"Translating Web Search Queries into Natural Language Questions Adarsh Kumar, Sandipan Dandapat, Sushil Chordia https://arxiv.org/abs/2002.0263 Abstract 사용자는 특정 질문을 염두해두고 검색엔진에 query하는데 이러한 query는 key…","fields":{"slug":"/NLP/translate_query/"},"frontmatter":{"date":"February 17, 2020","title":"[논문리뷰] Translating Web Search Queries into Natural Language Questions","category":"NLP","draft":false}}},{"node":{"excerpt":"Semantics-aware BERT for Language Understanding Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou https://arxiv.org/abs/1909.02209 Abstract 최근의 language representa…","fields":{"slug":"/NLP/sembert/"},"frontmatter":{"date":"February 07, 2020","title":"[논문리뷰] Semantics-aware BERT for Language Understanding","category":"NLP","draft":false}}},{"node":{"excerpt":"Background Transformer에서는 sinusoids 기반의 Position encoding을 사용하여 position 정보를 capture하였음. 또 다른 방법으로 BERT에서는 position embedding을 통해 position 정보를 capture함. Self-Attention with Relative Position Represen…","fields":{"slug":"/NLP/rel_pe/"},"frontmatter":{"date":"February 06, 2020","title":"Relative Position Encoding 간략 정리","category":"NLP","draft":false}}},{"node":{"excerpt":"An Opinionated Guide to ML Research John Schulman blog: http://joschu.net/blog/opinionated-guide-ml-research.html reddit: https://www.reddit.com/r/MachineLearning/comments/ewki8s/d_an_opinionated_gui…","fields":{"slug":"/Research/ml_research/"},"frontmatter":{"date":"February 03, 2020","title":"An Opinionated Guide to ML Research","category":"Research","draft":false}}},{"node":{"excerpt":"BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA Nina Poerner, Ulli Waltinger, Hinrich Schütze https://arxiv.org/abs/1911.03681 Abstract BERT는 relatio…","fields":{"slug":"/NLP/bertnotkb/"},"frontmatter":{"date":"January 01, 2020","title":"[논문리뷰] BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA","category":"NLP","draft":false}}},{"node":{"excerpt":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning https://openreview.net/forum?id=r1xMH1BtvB Abstract inpu…","fields":{"slug":"/NLP/electra/"},"frontmatter":{"date":"November 16, 2019","title":"[논문리뷰] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","category":"NLP","draft":false}}},{"node":{"excerpt":"Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut http…","fields":{"slug":"/NLP/mixreview/"},"frontmatter":{"date":"October 16, 2019","title":"[논문리뷰] Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models","category":"NLP","draft":false}}},{"node":{"excerpt":"PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang https://arxiv.org/abs/1910.07931 1. Introduction Pre-training Fine-tuning…","fields":{"slug":"/NLP/plato/"},"frontmatter":{"date":"October 05, 2019","title":"[논문리뷰] PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable","category":"NLP","draft":false}}},{"node":{"excerpt":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu h…","fields":{"slug":"/NLP/t5/"},"frontmatter":{"date":"October 04, 2019","title":"[논문리뷰] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","category":"NLP","draft":false}}},{"node":{"excerpt":"Language Models as Knowledge Bases? Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel https://arxiv.org/abs/1909.01066 1. Introduction 최근…","fields":{"slug":"/NLP/lama/"},"frontmatter":{"date":"October 03, 2019","title":"[논문리뷰] Language Models as Knowledge Bases?","category":"NLP","draft":false}}},{"node":{"excerpt":"Distilling The Knowledge of BERT for Text Generation Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu https://arxiv.org/abs/1911.03829 Short Review text generation task에 BERT를 사용하는것은 아직 의…","fields":{"slug":"/NLP/distill_knowledge/"},"frontmatter":{"date":"October 02, 2019","title":"[논문리뷰] Distilling The Knowledge of BERT for Text Generation","category":"NLP","draft":false}}},{"node":{"excerpt":"Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov https://arxiv.org/abs/1912.09637 Short Review zero-shot fa…","fields":{"slug":"/NLP/encyclo/"},"frontmatter":{"date":"October 01, 2019","title":"[논문리뷰] Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model","category":"NLP","draft":false}}},{"node":{"excerpt":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models Dogu Araci https://arxiv.org/abs/1908.10063 Abstract Financial sentiment analysis는 specialized language와 labled data가 없기 때문에 어렵고…","fields":{"slug":"/NLP/finbert/"},"frontmatter":{"date":"September 04, 2019","title":"[논문리뷰] FinBERT: Financial Sentiment Analysis with Pre-trained Language Models","category":"NLP","draft":false}}},{"node":{"excerpt":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut https://arxiv.org/abs/1909.11942 …","fields":{"slug":"/NLP/albert/"},"frontmatter":{"date":"September 02, 2019","title":"[논문리뷰] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","category":"NLP","draft":false}}},{"node":{"excerpt":"REPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, Dima Damen https://arxiv.org/abs/1908.08498 1. Introduction egocentri…","fields":{"slug":"/Multi_Modal/epic/"},"frontmatter":{"date":"August 02, 2019","title":"[논문리뷰] EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition","category":"Multi_Modal","draft":false}}},{"node":{"excerpt":"RoBERTa: A Robustly Optimized BERT Pretraining Approach Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy https://arxiv.org/abs/1907.10529 Abstract text span을 잘 표현하고 예…","fields":{"slug":"/NLP/spanbert/"},"frontmatter":{"date":"July 19, 2019","title":"[논문리뷰] SpanBERT: Improving Pre-training by Representing and Predicting Spans","category":"NLP","draft":false}}},{"node":{"excerpt":"RoBERTa: A Robustly Optimized BERT Pretraining Approach Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov https://arxi…","fields":{"slug":"/NLP/roberta/"},"frontmatter":{"date":"July 17, 2019","title":"[논문리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach","category":"NLP","draft":false}}},{"node":{"excerpt":"SegTree Transformer: Iterative Refinement of Hirarchical Features(ICLR19) Zihao Ye, Qipeng Guo, Quan Gan, Zheng Zhang https://rlgm.github.io/papers/67.pdf Abstract Transformer의 building block은 node가 …","fields":{"slug":"/Graph_Neural_Network/segtree_tf/"},"frontmatter":{"date":"July 07, 2019","title":"[논문리뷰] SegTree Transformer: Iterative Refinement of Hirarchical Features","category":"Graph_Neural_Network","draft":false}}},{"node":{"excerpt":"Growing a Brain: Fine-Tuning by Increasing Model Capacity(CVPR17) Yu-Xiong Wang, Deva Ramanan, Martial Hebert https://ieeexplore.ieee.org/document/8099806 Abstract 최근의 visual recognition system은 Imeg…","fields":{"slug":"/Transfer_Learning/growing_a_brain/"},"frontmatter":{"date":"July 01, 2019","title":"[논문리뷰] Growing a Brain: Fine-Tuning by Increasing Model Capacity","category":"Transfer_Learning","draft":false}}},{"node":{"excerpt":"XLNet: Generalized Autoregressive Pretraining for Language Understanding Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le https://arxiv.org/abs/1906.08237 1. In…","fields":{"slug":"/NLP/xlnet/"},"frontmatter":{"date":"June 01, 2019","title":"[논문리뷰] XLNet: Generalized Autoregressive Pretraining for Language Understanding","category":"NLP","draft":false}}}]}},"pageContext":{}},"staticQueryHashes":["3128451518","96099027"]}