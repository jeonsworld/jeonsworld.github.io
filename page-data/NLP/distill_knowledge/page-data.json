{"componentChunkName":"component---src-templates-blog-post-js","path":"/NLP/distill_knowledge/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","author":"[Jeonsworld]","siteUrl":"https:jeonsworld.github.io","comment":{"disqusShortName":"","utterances":"jeonsworld/blog-comment"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"812633db-9bf9-52f2-a219-94f39bb3b14f","excerpt":"Distilling The Knowledge of BERT for Text Generation Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu https://arxiv.org/abs/1911.03829 Short Review text generation task에 BERT를 사용하는것은 아직 의문점이 있음. 이를 해결하기 위해 text generation dataset에서 BERT를 fine-tuning할 수 있는 새로운 task…","html":"<blockquote>\n<p><strong>Distilling The Knowledge of BERT for Text Generation</strong><br>\nYen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu<br>\n<a href=\"https://arxiv.org/abs/1911.03829\">https://arxiv.org/abs/1911.03829</a></p>\n</blockquote>\n<h1 id=\"short-review\" style=\"position:relative;\"><a href=\"#short-review\" aria-label=\"short review permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Short Review</h1>\n<ul>\n<li>text generation task에 BERT를 사용하는것은 아직 의문점이 있음.</li>\n<li>이를 해결하기 위해 text generation dataset에서 BERT를 fine-tuning할 수 있는 새로운 task인 Conditional Maksed Language Modeling(C-MLM)을 제안.</li>\n<li>fine-tuned BERT를 seq2seq model로 distill함으로써 일관성있는 text generation을 위한 sequence-level supervised learning을 시도.</li>\n<li>제안한 방식은 NMT, Summarization에서 transformer baseline보다 좋은 성능을 보여줌.</li>\n<li>또한 IWSLT 독일어-영어, 영어-베트남어에 대한 SotA를 달성.</li>\n<li>본 논문의 접근방식은 BERT의 parameter을 직접 사용하지 않는다는 점이 related work들과 큰 차이점.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB90lEQVQoz1VTyW7bMBD1/39Dc0h7SIH00mNRoOghQdMViVPElmrZhmXJ2kWRFElRll6Hat3GAzxxhhrN8mY0A0nf9wQLoxWahoFzBms7KNWiYWRLMUGT7e7b1p0WXdfhJOM4TufMKUopiNaiqBS2YT6hkRZ5wRAfEkSrAJHvI8kqsEYgTROsN1t4no9DkmAYhn9BZy6wyyzzB6jiB7p6Dl0+oIq+oI4+Q+TfoA93MPEnyJTsagfTHVFXOVWq0XBJ1fbnAY2WGIu3GA6vMKZXMOFLFMsXKL0L1P4FxuwKQ/YaSC5h2T20sRhUSBEYRsuIrmcVOsW13CmOo6FsWkx6HIXYbAJw4lBLDdM6GEhB/PUj+TEomaHTnOzjs4D0aJXG417i7leB20WK75sGaV4gTwk8wTK+hxfPsS6fUDQJtKaBGQNhRhwHnFc4/g34cVni+sbHG8L7nxn64c/UMrHH1/UH3Dy+wzy6RdI4Di0yCixMCeE6ssfzli2N36GuGZhrkSg4iaOjqkqaLnFF5HfEX9sqcCHBOIckfcR/mbl9ch+VZYUw3GO3C5FTuy4wbzitSIpVsMJ2u6FVWSOOY/ILsd+Hk14UBQy1r7We9nLmeR6Rv8bTYoEgCOD7HjmVlKCcnDlV4XSXoK7rCac7KSX9CA11UE3vnf0bQKz2PWfcEhQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig1\"\n        title=\"fig1\"\n        src=\"/static/1837ae3049e849f48af65a8bc557459b/fcda8/fig1.png\"\n        srcset=\"/static/1837ae3049e849f48af65a8bc557459b/12f09/fig1.png 148w,\n/static/1837ae3049e849f48af65a8bc557459b/e4a3f/fig1.png 295w,\n/static/1837ae3049e849f48af65a8bc557459b/fcda8/fig1.png 590w,\n/static/1837ae3049e849f48af65a8bc557459b/bad1b/fig1.png 841w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<h2 id=\"fine-tuning-bert-with-conditional-mlm\" style=\"position:relative;\"><a href=\"#fine-tuning-bert-with-conditional-mlm\" aria-label=\"fine tuning bert with conditional mlm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Fine-tuning BERT with Conditional MLM</h2>\n<ul>\n<li>seq2seq는 left-to-right 방식이기 때문에 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi></mrow><annotation encoding=\"application/x-tex\">t</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span></span></span></span>번째 token <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub></mrow><annotation encoding=\"application/x-tex\">{y}_{t}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 예측하기위해 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{y}_{1:t-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mrel mtight\">:</span><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span></span></span></span>만 볼 수 있음.</li>\n<li>이러한 single directional limitation을 보완하기위해 target generation task에서 BERT를 fine-tuning할 수 있는 C-MLM 제안.</li>\n<li>fine-tuned BERT를 통해 더 나은 text를 generation할 수 있음.</li>\n<li>knowledge distillation을 사용하여 fine-tuned BERT에서 배운 knowledge를 text generation을 위한 seq2seq model로 distillation함.  </li>\n<li>fine-tuned BERT(teacher)는 텍스트 생성을 위한 기존의 seq2seq 모델(student)을 개선하기 위해 추가 감독으로 활용.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 401px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.37837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7ElEQVQ4y31UaXeiShT0//+lySyZjFviLoIRUcEouAJqXF6WelWXN/n4+pw+QHO7blXd2106Ho8IwxDz+RwvLy+YTqdYLBZYr9c4n894fX21eTqdcDgcsNmskSSJfWdZhiVj9/sUt9sNGqXNZgPXHcBxHHieh1a7jYeH33AGA+jfarWyp0CUULHecIjL5YLtbgun30OcrJgsLwC32y3SNDU2YqDAoeeiTeBarY6HP2U0Gw08cdZrNVSrFUymM0hZnudktzc1Ofca4G63w4DsqtUqKpUKGk+PZDpEp9NBs9WG/n9+fpokvYuhSyWSuyb7h9/3puZ8vhSAyrIjSwF2ez3z8Pn52ZIcjycLEuDb2xt2ZNNkwlq9TjVHxMsFfvz4idHIx+V6LQBFN2U2URe4CnJ3d4capZUrVfy6v0etUjaQ79xcpewG5StWdhlTYdC2Lw/Hvg9/NMKA1P3xGJPJFN/vvvHbpfF725RlOZbLJfr9vhVQBLbbDa3p2n51ggEqODCQCf1xMZtNkcQJ3IGDySwyuX+HWkVeqoAaWZYSvOiGv2sGuHhZoEf/RmTpDvrw/THGTNLrO7hei/6Sh4f/Kql3jTzPMGQL7Sn3i6G0a3E49KwgA6ePcRCY7CAYs40Ksz8+PqxVNN7f3+2Zpnv0ul2z5QtQlfQozx8HlDuzFlDbqPccguf8f6EcgcnvK6t5Oh1Nuprdp39btpP61wDFLooiHFi12+0fyp8j4jGUjIxzydbQBoElcWxKZrMQIfdIcsy1mMXSiZLfpafHOlqtlgWsVmtWscfvthVJYGIQ0AKPBVOiTqdtfSoiWu+0W/we0v8xvX1H6ZkZVZBgUvinn2oXTTESsGKicIY5i+e5HmNG6NK7ZrPJs+wYmRkvGHlbajNDuVy2jSNmHhGww+CnRtNOi3pObBbzkOs9TIKJXQYLXhRRFPL0pHYk5acBhixEZH4UB13X2JzBupZiBiX0JklinqDiasvyHP83/gUS2ZfZHTxJFwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table1\"\n        title=\"table1\"\n        src=\"/static/22be662adff67f511c277601df9cadb4/9144d/table1.png\"\n        srcset=\"/static/22be662adff67f511c277601df9cadb4/12f09/table1.png 148w,\n/static/22be662adff67f511c277601df9cadb4/e4a3f/table1.png 295w,\n/static/22be662adff67f511c277601df9cadb4/9144d/table1.png 401w\"\n        sizes=\"(max-width: 401px) 100vw, 401px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 398px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 78.37837837837837%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACzElEQVQ4y3VUiW7iQAzN/39UaZeyLFCuQC7IASFcARIgoYfa8vbZQNWutCNZyszYb+zn5xhFccRyucR6vcZiscBiPtf9fr/H8/MzTqfTl8nZdrul35z+KY7HIzabDeYas0JRFDDWqyVs20Y0mWA4HKLd6aDVamHs+wwosNvtkOe5Bs9mM/R7PTQbDYzGPsryBH/socMY2a/TDQGZ2Wq14ibF4XDQVySLeu0R3W4P9/cVVKuPqNV+02rcP6Ber2NxrUJ8Mz4qmQqWsUnX+sIfvrrfH/D29qb2/v7+wz4+PjTLZDaFH4RKi6xZHDPjP7BdT++Ni9MM7acWWk9t5cN1HJiDgQJ/X8JjPIkUUKqStVwuMB55mExjvTcKAo540O+bME0TtjUkf6GCeqMR+dv/AEziqXIeJ/MLIBvkug7iWXIBFB4Cf4zK3R0eflXheS6566rtdtlXuWLC8XjkovpYRZcJyHIILpyag6HeG9LFko04Hg/aTWmK69ioVO4xoJNp9rU50t0uue7woWazqWULJQfyLkD5tUGUzQqWZVEylkrk5eVFgedXPQpH0+lU91EUqVZjNkKCRafb7YaSGSPL8gvgbpMqYLvd0bafz2f8b5Vlefn45iOV9Zi9aFKFvaV+hFQpI5kvrv7nH8C3bynt8/NT+bydHXnWabdZMjXMBw1JM0kSLU10WJbF18hJRmK30RNf2UsmN588yxAEAVJOiepwRZ5E2EtyKcTKbA/YiCCMLurneRRNdMYXnNdJFCrfQ8vmKMaqxygM4FBmOnpzZieAwuOIuhMQnyRbDPA8j8EDPduzUSEfCRnsc85lGMLAh8MJ8Vyb2nQQUdxGul6hQf5s2+LrEUUdqHBF1HLW4EjK3AotMbs9oPg7nS5KlluwIfIPkFJFHdIY43Qqyc0Or6+vanKZXn8UwlHKWc+YnWQpd3meKZev/4zlbf0FLZeg8Rxz5NkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table2\"\n        title=\"table2\"\n        src=\"/static/98a6cdc85e1f469e7d78ab4848d9b86c/692d4/table2.png\"\n        srcset=\"/static/98a6cdc85e1f469e7d78ab4848d9b86c/12f09/table2.png 148w,\n/static/98a6cdc85e1f469e7d78ab4848d9b86c/e4a3f/table2.png 295w,\n/static/98a6cdc85e1f469e7d78ab4848d9b86c/692d4/table2.png 398w\"\n        sizes=\"(max-width: 398px) 100vw, 398px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 424px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 73.64864864864865%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACwUlEQVQ4y21U13LiQBDU/3/V+XzYJAEiKIBAEhI5Z+OC6+seym+3VVuSVhN6enrWOR6PmE6nmEwmyPMx8qKw7/P5jPv9jq+vL3terldsNxukaYpxlmG73UG++k6SxHwulwuc+XwG3/fR6/XgczdbLbS44zjGbD5nkhz5eIyUQYJeBx+fn6jX6/y3wGazRhiG5qfnYrGEs1wuMafjbrfDarVi5i32+x3Knx8oV2qoVsp8L8NtNFCvVVEqfaDV7hg67e12Y37r9dp8HQWKwgDv7++o1mrodDqGOAhCM/pZ39/feP79iygK0Y8iUnCz82HcR6VaY+kZrqTF2ZCXyaQwFII9Go2s5EazSaR7zGYzrIkgzwuk5Ort7RdRlrBab4zfeNBHu93GKElxOBxeAeWo6MfjyYgVDWVy1SXaWt2F12rCY5mtZgOe1zYOVdntdsOJZSuQfHVmHMqwUqmac4NcqWTXddEl2ZfLFY/Hw/i5E5HK1bnQiYYkGVlly9XaOHWEbjzO0O/30R/EGA6H1lkFnVJKj+fTuDqfL/aUvKak4Wft2JQxVSBOTTZ7wux22gw0Ih8DhEQQBC+EfhDgTIRPBj2fT+h1O/j9p2SSUlKtKPTZzDr2hyNOpxMcCXLAQOqo5KNsCcW64Lv43bFUlStqZCcRz2avQdBZwUGQXvVuTVG2GuUiA+lpnKWGcs6yej3fJkE0ZBI2Eb+CjiifiN31yF+AWNXFA6PJkbG6qe7JQRmltbbnmYGk4jaaFkQ0uG4DAYNkTNSnnUfJSGaqSNw6KkEchmFkelTGgF1L08RQReSzziApqVACbQGQjcD4vS65jymtFrtPhOquzTCFrHnNmGBAGjQpEZ2alJHQDkcJ0abWOJUqYesCWS0X5qfKCjbKUSPmHHSRmtBBl0VR5Lw9ZtagH5loYlY2MTn/T+wG+t/6B0RWREARuZgIAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table3\"\n        title=\"table3\"\n        src=\"/static/751f209b345bd8bb2b1df53b0b90085a/1cfa9/table3.png\"\n        srcset=\"/static/751f209b345bd8bb2b1df53b0b90085a/12f09/table3.png 148w,\n/static/751f209b345bd8bb2b1df53b0b90085a/e4a3f/table3.png 295w,\n/static/751f209b345bd8bb2b1df53b0b90085a/1cfa9/table3.png 424w\"\n        sizes=\"(max-width: 424px) 100vw, 424px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>","frontmatter":{"title":"[논문리뷰] Distilling The Knowledge of BERT for Text Generation","date":"October 02, 2019"}}},"pageContext":{"slug":"/NLP/distill_knowledge/","previous":{"fields":{"slug":"/NLP/encyclo/"},"frontmatter":{"title":"[논문리뷰] Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model","category":"NLP","draft":false}},"next":{"fields":{"slug":"/NLP/lama/"},"frontmatter":{"title":"[논문리뷰] Language Models as Knowledge Bases?","category":"NLP","draft":false}}}},"staticQueryHashes":["3128451518","96099027"]}