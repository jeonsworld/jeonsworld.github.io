{"componentChunkName":"component---src-templates-blog-post-js","path":"/NLP/bertnotkb/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","author":"[Jeonsworld]","siteUrl":"https:jeonsworld.github.io","comment":{"disqusShortName":"","utterances":"jeonsworld/blog-comment"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"f07c3e37-4a65-5900-996c-d2a2ab78ee50","excerpt":"BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA Nina Poerner, Ulli Waltinger, Hinrich Schütze https://arxiv.org/abs/1911.03681 Abstract BERT는 relational fact에 대한 cloze-style question에 대해 능숙함. LAMA에서는 BERT가 pre-training동안 factual…","html":"<blockquote>\n<p><strong>BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA</strong><br>\nNina Poerner, Ulli Waltinger, Hinrich Schütze<br>\n<a href=\"https://arxiv.org/abs/1911.03681\">https://arxiv.org/abs/1911.03681</a></p>\n</blockquote>\n<h1 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstract</h1>\n<ul>\n<li>BERT는 relational fact에 대한 cloze-style question에 대해 능숙함.</li>\n<li>LAMA에서는 BERT가 pre-training동안 factual knowledge를 암기한다고 주장.</li>\n<li>이러한 해석에 문제를 제기, BERT의 성능이 entity name을 reasoning하기 때문이라고 주장.</li>\n<li>easy-to-guess fact를 필터링할때 BERT의 정밀도가 떨어짐을 보여줌.</li>\n<li>entity mention을 symbolic entity embedding으로 대체하는 E-BERT를 제안.</li>\n</ul>\n<h1 id=\"1-introduction\" style=\"position:relative;\"><a href=\"#1-introduction\" aria-label=\"1 introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Introduction</h1>\n<p>많은 상식을 알고있다고 주장하는 친구가 있다고 상상해보자.</p>\n<ul>\n<li>Q1: Jean Marais의 모국어 A1: Fench</li>\n<li>Q2: Diniel Ceccaldi(프랑스 배우지만 italian-sounding name)의 모국어 A2: Italian  </li>\n</ul>\n<p>이것이 QA benchmark인 경우 50%의 정확도이다.\n그러나 performance는 배우의 모국어에 대한 factual knowledge를 나타내지 않음\n사람들 이름의 기원에 대해 reasoning할 수 있음을 보여줌\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 377px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50.67567567567568%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB2ElEQVQoz21S2Y7aQBD0/3/S5iXJ28JijO8LjA22WTy+QVyCSs1I3miVtDQj9VXdXd0aKK/XC/f7/es9Hg9cr1dcLheURQHLXMEPIrRtAyFqZOkWq5UBx3VR17WKnXA0+Umpa4HtdovNZoM4jnA8HjEMA6IohG1ZsPjSNFXP81wCruiLkGUZ+r7/F7Bn9cVCh2PbMC2T4Ck7anE4HOC5DlzPU3EDk2MWMQnY9SMmmXD+AnYNTNNCGIaqsk3gKF6jbWo1smnZqJtGjbhex8pfN+03sG8ddm2NpbGCYRjQFx/Y73PcbncU+Z6FTLy/z5Dnueo6DHzGLVGJRuU+n08FpgCnliXhMqGqKpRlqRIlh5MuR5c2QV3apC7Yrez4fD5/ja4VBJFjdv2AWlRcSsLKArtdhmSboChKblYgJfkZF3L4/MSBBSSYBFY+bn3HicbTCVoQBHh7+8ER94o71/XwMZ/j96+fMMjpguP7fqA2uVzqilfHtuDR5jgODNpmszm3biLniWkJCfaDEBk70vWl4ivwfTpzJMmGNl0BxnFMEB9rnpRBro/HClfeqRCVus0Tu7vdbtDGoccwjujJV9d15EhgpC6PteVWJW/ywKejPp9PjOvV8f9P/gAmqObzYdVDugAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table1\"\n        title=\"table1\"\n        src=\"/static/92e9e04986fa88ab1155a341f2ef2a61/6146e/table1.png\"\n        srcset=\"/static/92e9e04986fa88ab1155a341f2ef2a61/12f09/table1.png 148w,\n/static/92e9e04986fa88ab1155a341f2ef2a61/e4a3f/table1.png 295w,\n/static/92e9e04986fa88ab1155a341f2ef2a61/6146e/table1.png 377w\"\n        sizes=\"(max-width: 377px) 100vw, 377px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<p>LAMA에서는 unsupervised BERT LM이 factual knowledge를 암기한다고 주장.\nBERT가 relation extraction에 의해 구축된 knowledge base에 필적하는 unsupervised QA benchmark LAMA에 이 진술을 근거로 함.\n그들은 BERT와 유사한 LM이 “text로부터 추출된 기존의 knowledge base에 대한 대안”이 될 수 있다고 주장.\nBERT의 성능은 부분적으로 entity name의 reasoning에 기인한다고 주장.  </p>\n<p>section 2.1에서는 entity name만으로 쉽게 응답할 수 있는 quert를 filtering하여 LAMA-Google-RE 및 LAMA-UHN 구성.(UnHelpfulNames, LAMA-T-REx의 actual” 하위집합)\nLAMA-UHN에서 BERT의 성능이 크게 저하됨을 보여줌.</p>\n<p>section 3에서는 entity mention을 wikipedia2vec entity embedding으로 대체하는 E-BERT를 제안.  </p>\n<p>section 4에서는 E-BERT가 BERT와 LAMA에서 최근 제안된 entity enhanced ERNIE와 경쟁한다는 것을 보여줌.  </p>\n<p>E-BERT는 LAMA-UHN의 두 기준선에서 실직적인 우위를 점하고 있음.\n또한, E-BERT와 BERT의 ensemble은 original LAMA의 모든 baseline을 능가.</p>\n<h1 id=\"2-lama\" style=\"position:relative;\"><a href=\"#2-lama\" aria-label=\"2 lama permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. LAMA</h1>\n<p>LAMA benchmark는 LM 고유의 “factual and commonsense knowledge”를 조사한다.\n본 논문에서는 factual knowledge을 목표로하는 LAMA-GoogleRE 및 LAMA-T-REx에 중점을 둠.\nQA에 대한 대부분의 이전 연구와 달리 LAMA는 w/o supervised finetuing으로 LM을 그대로 테스트함.</p>\n<ul>\n<li>\n<p>LAMA의 조사 방법은 다음과 같은 schema를 따른다.</p>\n<ul>\n<li>KB triple 형식(S,R,O)가 주어졌을때 cloze-style question(Jean Marais, native-language, French)으로 객체가 도출됨.</li>\n<li>“The native language of Jean Marais is [MASK].”</li>\n<li>LM은 gold answer에 대해 [MASK]를 대체할 limited vocaulary에 대한 distribution을 예측</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"21-lama-uhn\" style=\"position:relative;\"><a href=\"#21-lama-uhn\" aria-label=\"21 lama uhn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1 LAMA-UHN</h2>\n<p>entity에 대한 factual knowledge가 전혀없는 entity의 이름으로 entitny의 특성을 추측하는것이 종종 가능함.\nentity는 암묵적 또는 명시적 규칙.(작명을 하는데 관련된 문화적인 규범, 산업제품에 대한 저작권법)\nLAMA는 limited vocabulary로 추측하기가 훨씬 쉬우며, 특정 entity type 대한 일부 후보만 포함할 수 있음.</p>\n<p>entity name을 control하지 않는 QA benchmark가 추론에 도움이 되는지, fact를 암기하는데 도움이 되는지 또는 두 가지 모두에 도움이 되는지 평가하지 않는다고 주장.\n이 section에서는 LAMA-Google-RE, LAMA-T-REx의 subset인 LAMA-UHN(UnHelpfulNames)생성에 대해 설명.  </p>\n<h3 id=\"filter1\" style=\"position:relative;\"><a href=\"#filter1\" aria-label=\"filter1 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Filter1:</h3>\n<ul>\n<li>문자열 일치 필터(string match filter)는 answer(e.g., Apple)가 subject entity name(e.g., Apple Watch)의 대소문자를 구분하지 않고 하위 문자열인 모든 KB triple을 삭제.</li>\n<li>이 간단한 heuristic은 individual relation에서 triple의 최대 81%를 삭제. (부록 참고)</li>\n</ul>\n<h3 id=\"filter2\" style=\"position:relative;\"><a href=\"#filter2\" aria-label=\"filter2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Filter2:</h3>\n<ul>\n<li>entity name은 더 미묘한 방식으로 드러날 수 있음.</li>\n<li>프랑스 배우의 예시에서 알 수 있듯이 사람의 이름은 모국어와 확장, 국적, 출생지 등을 추측하기 전에 유용할 수 있다.</li>\n<li>개인이름 필터는 cloze-style question을 사용하여 BERT에 내재된 name associations을 이끌어내고 이와 연관된 KB triple을 삭제</li>\n<li>subject name을 Jean과 Marais 공백으로 tokenization.</li>\n<li>BERT가 두 이름 중 하나를 공통 프랑스 이름으로 간주하면 Jean Marais entity에 대한 factual knowledge 증거가 충분하지 않음.</li>\n<li>반면에 Jean 이나 Marais도 프랑스어로 간주되지는 않지만 정답이 제시되면, factual knowledge에 대한 충분한 증거를 고려함.</li>\n<li>“[X] is a common name in the following language: [MASK].”</li>\n<li>for both [X] = Jean and [X] = Marais.</li>\n<li>correct answer가 두 query에서 top-3중 하나인 경우 triple을 삭제.</li>\n<li>그림 1은(blue bars) BERT가 필터링에 의해 크게 영향을 받는것으로 나타났으며, LAMA에서 LAMA-UHN 평균 P@1이 5%~10% 떨어졌다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 390px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 83.1081081081081%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAADpklEQVQ4y62US4/bVBiG8wP4CeyQ2CBVSEgs+AV0RcWWDQIJ/gACIdQBCVQ0AoGAIo00vXARLWW6mDZSp3NpWqDtMEmTmUmcmVycOBfHuThO7EwcJ87l4bMXHXXBjiPZ5/ic4/d73+97z4kgbTKZhE/QvJHLeOEzns2YMpPxnLE3ZjQaMZM5mDP0x/iLhawt8MY+w+GQ+Xwe/h8JXpZloWkaXdOk2mhiP6zgKBnMVIPBowTuRIJ4noC6dB2P6rd/Uri2iX4hiq0dMRpPhMjoFNC2bQyjycRzuL/XZOuFH7m88h4rr1zkYPkdjlEQuix8j2T2hNhLF7lx9jOUM+fxqw+CJUbC8ilgp9OhWFTpdnR2duskX7zK15ffZPXlH4hfepufvljm4PsL1FBJp2c8OLPC7XNfor62zOCPS1h3/sGbuqeAQW58f8pi7pMr2hxcfUw8uUl+Tfp76/z1+0NKG3dIt9I0K4bMP2Jv7R76VoL92A77v9zGGXSfBfQkR9OpT88a0Rr2Gc0WzF2HjtmT6CMmDgydEfPJAMszMYcOtn+C6fdp9ayQ0FPAbrcrkotUtCoty2DecXGVNCey5k89huKAMRPJk4s3lwJUXBy1IUGnUoxgLSA0frYout6g025S0E1qS3F21z/ATuzhRG8wUVX8xxuiwKXRn5L5JMXJzSv0bsY5OYzjPakKsHMKaIpdymUNu98iGtOIPvcdP0fPcffsEve/eovYR0vE3n9Dap0kGR+z/vw37L77KdlXz6N9/Dm11z9kMKycAvq+j+MMpNotVK2NunbA3e3r7KzG+HvjNzavbaEltik6eYz6AOX6Lpurt8j8usPWlSiFm9vi0f4p4P/ZIpbILUqOWq0Wg8EgLE6tUpWTU6amVTC7FoauUymrFNQS1WpVxhpauUzh+Jhmsy3fZRqNBnrDIGL3e6QzGQ4PD6UwOk8SCUqlEvl8PuzLsjkA0es1kql9mc+Ry+UEwBBXaBwJaEDo+CgbBoyY7TbHuXx4Wnq9HqpaDEGMZjPcVKvVQrbVWl082UHJpEMVXTn/wdlXhEy+UAz3tAQrYokHs3IRBGw0kTpwHFKpZAisSsRCsUBJxplMln7fDoNYVg9F/jnKKiREUWr/gHq9HgaPBKbeF4BMRuEwnZZcNkPpiqKEQQLgrIxzoiJIQ04kBkyCfNv9vrjDkUB9XNeVSntEAst0Om2aUpSOSGjL5uB+s0R+kIbA9Kb0wbxhGOF3QOK/2r+3jss5hEAbvwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"pig1\"\n        title=\"pig1\"\n        src=\"/static/34d5d83bb4123a2db83b8f3d0b8629e4/727ba/fig1.png\"\n        srcset=\"/static/34d5d83bb4123a2db83b8f3d0b8629e4/12f09/fig1.png 148w,\n/static/34d5d83bb4123a2db83b8f3d0b8629e4/e4a3f/fig1.png 295w,\n/static/34d5d83bb4123a2db83b8f3d0b8629e4/727ba/fig1.png 390w\"\n        sizes=\"(max-width: 390px) 100vw, 390px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<p>이것은 BERT가 entity name에 대하여 추론하기때문에 LAMA에서 부분적으로 잘 작동한다는 것을 나타냄.\n물론 name-based 추론은 그 자체로 유용한 능력이지만 factual knowledge로 그것을 혼란시키는 것은 오해의 소지가 있을 수 있다.</p>\n<h1 id=\"3-e-bert\" style=\"position:relative;\"><a href=\"#3-e-bert\" aria-label=\"3 e bert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. E-BERT</h1>\n<h3 id=\"wikipedia2vec\" style=\"position:relative;\"><a href=\"#wikipedia2vec\" aria-label=\"wikipedia2vec permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Wikipedia2vec</h3>\n<ul>\n<li>Yamada et al., 2016은 단어와 wikipedia page(entity)를 common space에 embedding.</li>\n<li>단어 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">L</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{{ L }}_{ w }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83889em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 와 entity <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">L</mi><mi>e</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{{L}}_{e}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83889em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대한 embedding function을 학습.</li>\n<li>\n<p>wikipedia2vec loss에는 세가지 구성 요소가 있음.</p>\n<ul>\n<li>(a) skipgram word2vec</li>\n<li>(b) wikipedia link graph의 graph loss</li>\n<li>(c) entity mention으로부터 word가 예측되는 word2vec</li>\n<li>loss (c)는 word와 entity embedding이 공간을 공유하도록 함</li>\n<li>그림1의 검은 가로 막대는 loss(b)가 중요하다는것을 보여줌</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"e-bert\" style=\"position:relative;\"><a href=\"#e-bert\" aria-label=\"e bert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>E-BERT</h3>\n<ul>\n<li>transformed wikipedia2vec word vector와 BERT subword vector의 squared distance를 minimizing 하는것으로 최적화함.\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><mi>n</mi></mrow><mi mathvariant=\"script\">W</mi></msub><mspace width=\"1em\"/><msub><mi mathvariant=\"double-struck\">E</mi><mrow><mi>x</mi><mo>∈</mo><msub><mi mathvariant=\"double-struck\">L</mi><mi>b</mi></msub><mo>∩</mo><msub><mi mathvariant=\"double-struck\">L</mi><mi>w</mi></msub></mrow></msub><msubsup><mrow><mo fence=\"true\">∥</mo><mi mathvariant=\"script\">W</mi><mrow><mo fence=\"true\">(</mo><mi mathvariant=\"script\">F</mi><mrow><mo fence=\"true\">(</mo><mi>x</mi><mo fence=\"true\">)</mo></mrow><mo fence=\"true\">)</mo></mrow><mo>−</mo><msub><mi mathvariant=\"script\">E</mi><mi mathvariant=\"script\">B</mi></msub><mrow><mo fence=\"true\">(</mo><mi>x</mi><mo fence=\"true\">)</mo></mrow><mo fence=\"true\">∥</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">{ argmin }_\\mathcal{{ W} }\\quad \\mathbb{{ E }}_{ x\\in \\mathbb{{ L }}_{ b }\\cap \\mathbb{{ L} }_{ w } }{ \\left\\| \\mathcal{W}\\left( \\mathcal{F}\\left( x \\right)  \\right) -\\mathcal{{ E }}_\\mathcal{{ B }}\\left( x \\right)  \\right\\|  }_{ 2 }^{ 2 }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.253708em;vertical-align:-0.29969999999999997em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\">a</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">i</span><span class=\"mord mathdefault\">n</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.23419099999999995em;\"><span style=\"top:-2.4558600000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathcal mtight\" style=\"margin-right:0.08222em;\">W</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.24414em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:1em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">E</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33222299999999994em;\"><span style=\"top:-2.55em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">x</span><span class=\"mrel mtight\">∈</span><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathbb mtight\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3487714285714287em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">b</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15122857142857138em;\"><span></span></span></span></span></span></span><span class=\"mbin mtight\">∩</span><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathbb mtight\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.16454285714285719em;\"><span style=\"top:-2.357em;margin-right:0.07142857142857144em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.25586em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">∥</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.08222em;\">W</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.08944em;\">E</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathcal mtight\" style=\"margin-right:0.03041em;\">B</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose delimcenter\" style=\"top:0em;\">)</span></span><span class=\"mclose delimcenter\" style=\"top:0em;\">∥</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.954008em;\"><span style=\"top:-2.4003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.29969999999999997em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">W</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{W}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.08222em;\">W</span></span></span></span></span>는 linear projection</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">F</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span></span></span></span>는 같은 space에서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">L</mi><mi>e</mi></msub><mo separator=\"true\">,</mo><msub><mi mathvariant=\"double-struck\">L</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{{ L }}_{ e }, \\mathbb{{ L} }_{ w }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.88333em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 포함하기때문에 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">L</mi><mi>w</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{{ L} }_{ w }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83889em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02691em;\">w</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 통해 학습하면 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"double-struck\">L</mi><mi>e</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbb{{ L }}_{ e }</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83889em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">L</span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">e</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에게도 적용됨.</li>\n</ul>\n<h1 id=\"4-experiments\" style=\"position:relative;\"><a href=\"#4-experiments\" aria-label=\"4 experiments permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Experiments</h1>\n<ul>\n<li>\n<p>Examples: The native language of Jean marais is [MASK].</p>\n<ul>\n<li>E-BERT: The naitive language of Jean_Marais is [MASK]</li>\n<li>BERT and E-BERT ensemble (AVG): mean-polling their outputs</li>\n<li>BERT and E-BERT ensemble (CONCAT): e.g.: Jean_Marais / Jean Mara ##is</li>\n</ul>\n</li>\n<li>그림 1에서 E-BERT는 factual knowledge를 통해 BERT보다 더 뛰어난 성능을 보여줌.</li>\n<li>GoogleRE중 46%는 entity가 포함되지 않음.(성능 차이가 크게 없는 이유)\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 390px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 83.1081081081081%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsTAAALEwEAmpwYAAADpklEQVQ4y62US4/bVBiG8wP4CeyQ2CBVSEgs+AV0RcWWDQIJ/gACIdQBCVQ0AoGAIo00vXARLWW6mDZSp3NpWqDtMEmTmUmcmVycOBfHuThO7EwcJ87l4bMXHXXBjiPZ5/ic4/d73+97z4kgbTKZhE/QvJHLeOEzns2YMpPxnLE3ZjQaMZM5mDP0x/iLhawt8MY+w+GQ+Xwe/h8JXpZloWkaXdOk2mhiP6zgKBnMVIPBowTuRIJ4noC6dB2P6rd/Uri2iX4hiq0dMRpPhMjoFNC2bQyjycRzuL/XZOuFH7m88h4rr1zkYPkdjlEQuix8j2T2hNhLF7lx9jOUM+fxqw+CJUbC8ilgp9OhWFTpdnR2duskX7zK15ffZPXlH4hfepufvljm4PsL1FBJp2c8OLPC7XNfor62zOCPS1h3/sGbuqeAQW58f8pi7pMr2hxcfUw8uUl+Tfp76/z1+0NKG3dIt9I0K4bMP2Jv7R76VoL92A77v9zGGXSfBfQkR9OpT88a0Rr2Gc0WzF2HjtmT6CMmDgydEfPJAMszMYcOtn+C6fdp9ayQ0FPAbrcrkotUtCoty2DecXGVNCey5k89huKAMRPJk4s3lwJUXBy1IUGnUoxgLSA0frYout6g025S0E1qS3F21z/ATuzhRG8wUVX8xxuiwKXRn5L5JMXJzSv0bsY5OYzjPakKsHMKaIpdymUNu98iGtOIPvcdP0fPcffsEve/eovYR0vE3n9Dap0kGR+z/vw37L77KdlXz6N9/Dm11z9kMKycAvq+j+MMpNotVK2NunbA3e3r7KzG+HvjNzavbaEltik6eYz6AOX6Lpurt8j8usPWlSiFm9vi0f4p4P/ZIpbILUqOWq0Wg8EgLE6tUpWTU6amVTC7FoauUymrFNQS1WpVxhpauUzh+Jhmsy3fZRqNBnrDIGL3e6QzGQ4PD6UwOk8SCUqlEvl8PuzLsjkA0es1kql9mc+Ry+UEwBBXaBwJaEDo+CgbBoyY7TbHuXx4Wnq9HqpaDEGMZjPcVKvVQrbVWl082UHJpEMVXTn/wdlXhEy+UAz3tAQrYokHs3IRBGw0kTpwHFKpZAisSsRCsUBJxplMln7fDoNYVg9F/jnKKiREUWr/gHq9HgaPBKbeF4BMRuEwnZZcNkPpiqKEQQLgrIxzoiJIQ04kBkyCfNv9vrjDkUB9XNeVSntEAst0Om2aUpSOSGjL5uB+s0R+kIbA9Kb0wbxhGOF3QOK/2r+3jss5hEAbvwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig1.png\"\n        title=\"fig1.png\"\n        src=\"/static/34d5d83bb4123a2db83b8f3d0b8629e4/727ba/fig1.png\"\n        srcset=\"/static/34d5d83bb4123a2db83b8f3d0b8629e4/12f09/fig1.png 148w,\n/static/34d5d83bb4123a2db83b8f3d0b8629e4/e4a3f/fig1.png 295w,\n/static/34d5d83bb4123a2db83b8f3d0b8629e4/727ba/fig1.png 390w\"\n        sizes=\"(max-width: 390px) 100vw, 390px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n<li>string match filter를 통해 BERT에 대한 E-BERT의 차이가 부분 문자열 응답과 관련이 높은것을 알 수 있음.</li>\n<li>ensemble model은 대부분의 relation에서 높은 성능을 보여줌.</li>\n<li>이는 BERT의 entity name에 대한 reasoning 능력과 E-BERT의 향상된 factual knowledge를 성공적으로 결합한 것.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 385px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.05405405405406%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC9UlEQVQ4y3VUyY7bRhDlb+boWz7FgQFf7GuC+BLYhg+5xU4cBIiBzMjwDExS4k6KFCVxqIWixE3ySHp51ZoZCzDcQLGbVd2vX22tgeNwOGC/36NtW2w2G3Rd9yCiO/8/CXWibxu0ux222y12lOPxCE2A6rrGaDRCEIQYjycoyxLL5RKLxRJZlmG1KlEsiztZoihWKKoNVk2H3HWR8qzneaiqClqSJApQgJp6g/tR1WvU2wrfHc0X4Oot8Oo3zGYFBoMBL15BS9NUAco8ma5REjONV5iPG6yiEovhHOnFB8S//o7s5V9Y/PIz8ifPcfPDTygf/Yjs6TOMxks4jqU808SlmnFL8xzTjzrKp49RvP4D9bseqjf/oP73GqtghnnPR34dItddZH/+B/99D1fvL+B+spCT4XpdqlxocRxjQ+RJUaL6+Dfw4jFIGejo7r6TlH3r7s0XMs8QTGK0W0lco9QqKRL8kr7HZLq4vMTRNHBL4/5wxLIoIBfGw5MkXCejFN1shJskhG071A/Jbv0VUBZSCpKhOIrUJsdx0DdNFWjf92FybRgGLMui3YZhfqatr/aJTkrrAVA+D5klqKRfEiRAYRgqEZb3YLIOw0jti0hAPLwHk6FJIO8VB9ak1OX5kH/ZI/bzy8+HOss9KilD3tIfWJhKDOdzxURYjMdjuK5zYhn46j/LbhDwX9z3fZfNkCqmZr+vvKqYTM00+3A9H5PJCSDgYQW+WKiAX/Z6BAwYDnZTHMIc2JBmkJA4Nksmn/HsBCGJdd0WWhSFipUrsRslLO4pfI/Btl2CztHn7RFZOQ4ZJSPohq7aMSCgsMtnM1UNnucy2xto0sOGYSo3AzKZEtC2TwkQN2WeMxT+nU0yK+6ZdDuKE2SiYz+7FNUpAnR9dUWFB0PXWQ6WitGQzM3+ALY1UKByscUykgdEYqrrLCX9M8E8NE2j5Pb2Fpo8PTXLpWlaPg71aaZxx2dpd/c0bWjfslaFQduyM2ivKRVfHJnPx/+EQaFU4g107QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig2.png\"\n        title=\"fig2.png\"\n        src=\"/static/b92444da10750204324c7af25343f741/409e6/fig2.png\"\n        srcset=\"/static/b92444da10750204324c7af25343f741/12f09/fig2.png 148w,\n/static/b92444da10750204324c7af25343f741/e4a3f/fig2.png 295w,\n/static/b92444da10750204324c7af25343f741/409e6/fig2.png 385w\"\n        sizes=\"(max-width: 385px) 100vw, 385px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n<li>FewRel relation classification dataset에서도 E-BERT가 BERT보다 좋은 성능을 보여줌.</li>\n<li>ensemble model은 ERNIE와 비슷한 성능을 보여줌.</li>\n<li>ERNIE와 KnowBERT는 architecture에 새로운 parameter를 추가하여 추가 pre-train을 수행해서 통합 되어야 함.</li>\n<li>그러나 E-BERT는 통합 단계 없이 기존의 Pre-trained BERT와 함께 사용가능.</li>\n<li>본 논문에서 most expensive operation은 CPU에서 몇 시간정도 소요되는 wikipedia2vec.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 368px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.75675675675676%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACAklEQVQoz41Ta5OaQBDk//+iJFVXqZx3sSpn5FTwgaCgK8tDUFB8a1/P3sckVaFqanZhp6ent7EulwvyPMd6vf6vSNPUhKyTJDE5yzJorXE4HGDdbjecz2cTp9PJvGya5o+Q91VVISVIURQ8f0IURfx2IKEMq9UK9/udgNcrXNeB6ziIdYL9fv9XUHm33W4xmUwwn4eGped5JkdhiJDgQsgwHI/HeGm1YNs2iyr865ECaT4LAo6ZG/Cc4y4I5vsBLiRnAQ8Evo8RQSX3ej02mJD+w4A8Hg8T8hzJ0nEGCGYzMssI7mJN/efzuWF75n0YhoE/NSP3BwPzwfNGGDh9ShALJMFvJjfNHt+fnvD6sw2fLL99/YIZwbudN/x4fsaOelpX0nQINugLgEYcJ+y8ZFdNXVLeZAF/qslKm8vwOOZ0OuW52GQZWTQcDofUv/lk+Pb2C68vLTjuiNcfw+4qtNtLjs5xwpKFGTodxZtMqPNvuCyW5rbdpZYptRzj/b2Hw/H8CSgeiqKQsaA2CQs1RY6551oJyxxKiTViLBcLKFokSVKEZCZelHrNfDweYe3qCmq5QL1rUFdbjqK5rtlZ/LZm5NhsSuO1sixp5MLsK56t69r8FGJspZTZWzUBxQYpbSA3JmbNmEUjOSTdE4ZSKxNi4IJ/R7nZGDAhIGeEbVlu8AGiwDDXarICrAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table2.png\"\n        title=\"table2.png\"\n        src=\"/static/b751d1c24a250165d1c4a78ee48933a3/2b727/table2.png\"\n        srcset=\"/static/b751d1c24a250165d1c4a78ee48933a3/12f09/table2.png 148w,\n/static/b751d1c24a250165d1c4a78ee48933a3/e4a3f/table2.png 295w,\n/static/b751d1c24a250165d1c4a78ee48933a3/2b727/table2.png 368w\"\n        sizes=\"(max-width: 368px) 100vw, 368px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n</ul>\n<h1 id=\"appendix\" style=\"position:relative;\"><a href=\"#appendix\" aria-label=\"appendix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Appendix</h1>\n<h2 id=\"lama-uhn-filtering-결과\" style=\"position:relative;\"><a href=\"#lama-uhn-filtering-%EA%B2%B0%EA%B3%BC\" aria-label=\"lama uhn filtering 결과 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>LAMA-UHN filtering 결과</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.08108108108108%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACZUlEQVQ4y2WUB5PqMAyE8/9/HO0oqbQEUiAkwNGLTp9CZu7eM6Nxotjr3ZWMIzoul4tst1t5Pp/yfr8trterVFWlUctut7Moy9LytkZ/r9fL9rRB3gFwv9/LaDSSyWQs8/lczuezbDYbWSWJhGEoX19f4rquhmczuTRNbd/9fpfb7WZhgJxSFIUCDqXX68lqtZLj8WiM8yyTIPCl1+/LeDwW3/dtnk5nEkWRHA4H+Xc4nNAyYcNJ2THqupYsXdtGT/NRFCrQ1N5zJWCMdF1rURvO4/GQJIlNMoB4xMCzlebDMJLhcKh2TEyu5/kGjKrX621r/wCSgHpf5XqeZ75wOgxhTq7b7clgMJC+Sgd4sVhIEsdWpP8kg1pVO3F142TiSqwL1+u15HlhMxL5xgwzrKFwqfpLYVgDIZRRDwfadV3ZRwrSFoWW2WqlkYnkRq5nDKk6BSSHksvlbPuwzyTTJnhS0WsanHQ6neSg8gEYKABy6QSkD4cjYzubzUwJgzajF60otEemAX3YMYzlrjQLmiqrZA0kJ+rt7wEBmL4oCm0DWJ7nZjJSGd8fhshsZDddEPieeriwLgCorTIYDUMF3KhcKBMAYgGAx8Pe/Ot0Olbptn263e4flgBByADNQ92cZfmH5VZKvSUAfh8PyiowZjANAn3+FAYPseX51PustpXlrmkbtAO0XC61UmvzjNOt4p+Z9zheykLXsA6vmQFNfq3hAAeTuZ9jvSn0W3tnXWVBzr5pNDdFWeJj0LCGrffZ47oTPSQWZ/9hiKm0TlFsbN6obHvPGytSZUWu/WehOygo0T7j4Q+T7KotNyn66gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table3.png\"\n        title=\"table3.png\"\n        src=\"/static/c59e27d4d50baf7eab56227909b0b0e1/fcda8/table3.png\"\n        srcset=\"/static/c59e27d4d50baf7eab56227909b0b0e1/12f09/table3.png 148w,\n/static/c59e27d4d50baf7eab56227909b0b0e1/e4a3f/table3.png 295w,\n/static/c59e27d4d50baf7eab56227909b0b0e1/fcda8/table3.png 590w,\n/static/c59e27d4d50baf7eab56227909b0b0e1/108f8/table3.png 777w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>","frontmatter":{"title":"[논문리뷰] BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA","date":"January 01, 2020"}}},"pageContext":{"slug":"/NLP/bertnotkb/","previous":{"fields":{"slug":"/NLP/electra/"},"frontmatter":{"title":"[논문리뷰] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","category":"NLP","draft":false}},"next":{"fields":{"slug":"/Research/ml_research/"},"frontmatter":{"title":"An Opinionated Guide to ML Research","category":"Research","draft":false}}}},"staticQueryHashes":["3128451518","96099027"]}