{"componentChunkName":"component---src-templates-blog-post-js","path":"/NLP/finbert/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","author":"[Jeonsworld]","siteUrl":"https:jeonsworld.github.io","comment":{"disqusShortName":"","utterances":"jeonsworld/blog-comment"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"37c941c7-8a33-5300-859f-2ee857587b2d","excerpt":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models Dogu Araci https://arxiv.org/abs/1908.10063 Abstract Financial sentiment analysis는 specialized language와 labled data가 없기 때문에 어렵고 general-purpose model은 domain에서 사용되는 specialized language때문에 효과적이지 않다. pre…","html":"<blockquote>\n<p><strong>FinBERT: Financial Sentiment Analysis with Pre-trained Language Models</strong><br>\nDogu Araci<br>\n<a href=\"https://arxiv.org/abs/1908.10063\">https://arxiv.org/abs/1908.10063</a></p>\n</blockquote>\n<h1 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstract</h1>\n<ul>\n<li>Financial sentiment analysis는 specialized language와 labled data가 없기 때문에 어렵고 general-purpose model은 domain에서 사용되는 specialized language때문에 효과적이지 않다.</li>\n<li>pre-trained model은 더 적은 labled data를 필요로 하고 domain specific corpora에서 train할 수 있기 때문에 이 문제에 도움이 될거라고 가정.</li>\n<li>financial domain에서 NLP task를 처리하기 위해 BERT기반 language model인 FinBERT를 제안.</li>\n<li>FinBERT는 2가지 financial sentiment analysis task에서 SotA를 달성.</li>\n<li>FinBERT는 smaller training set과 model의 일부만 fine-tune해도 기존의 SotA보다 우수한 것을 발견.</li>\n</ul>\n<h1 id=\"1-introduction\" style=\"position:relative;\"><a href=\"#1-introduction\" aria-label=\"1 introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Introduction</h1>\n<p>open market의 가격은 경제에서 교환되는 자산과 관련하여 사용 가능한 모든 정보를 반영.\n뉴스, 분석가의 보고서, 공식 회사발표 등 financial text analysis는 새로운 정보의 source.\n전례없는 양의 text가 매일 만들어짐에 따라 이러한 text를 수동으로 분석하고 그로부터 actionable insight를 얻는것은 single entity에게는 너무 큰 과제임.  </p>\n<p>이에 따라 NLP의 방법을 사용하여 financial actor가 작성한 text의 automated sentiment analysis는 지난 10년동안 큰 인기를 얻었다.\n본 논문의 주요 연구 관심사는 polarity analysis(긍,부정 classification 같은 예를 말하는거같음)이다.  </p>\n<p>이를 위해 두가지 task를 해결해야 함</p>\n<ol>\n<li>neural network를 사용하는 classification method는 많은 양의 labled data가 필요하여 financial text에 lable을 지정하려면 전문 지식이 필요.</li>\n<li>financial text에는 unique vocabulary를 가진 specialized language가 있고 쉽게 식별되는 긍/부정 단어 대신 모호한 표현을 사용하는 경향이 있기 때문에 일반적인 copora에 대해 학습된 model은 적합하지 않음.</li>\n</ol>\n<p>Loughran and McDonald(2011)는 신중하게 만들어진 financial sentiment lexicons를 사용하여 해결책으로 보일 수 있으나 이러한 방식은 text의 semantic meaning을 분석하는데는 부족함.  </p>\n<p>NLP transfer learning은 위에서 언급한 두개의 task에 대한 solution으로 보이며 논문의 이 초점을 맞추고 있는 점이다.\n핵심 아이디어는 매우 큰 corpora에서 language model을 학습하고 language modeling task에서 배운 가중치로 downstream task를 초기화함으로써 훨씬 더 나은 성능을 달성할 수 있다는것 이다.</p>\n<ul>\n<li>\n<p>본 논문의 main contribution은 다음과 같음</p>\n<ul>\n<li>financial NLP task를 위한 BERT기반의 language model인 FinBERT를 제안하고 두 가지 financial sentiment analysis dataset에서 성능을 검증.</li>\n<li>FiQA sentiment scoring과 Financial PhraseBank에 대해 SotA를 달성.</li>\n<li>financial sentiment analysis를 위한 pre-trained language model ULMFit, ELMo를 구현하고 이를 FinBERT와 비교.</li>\n<li>\n<p>다음을 포함하여 모델의 여러 측면을 조사하기 위한 실험을 수행.</p>\n<ul>\n<li>financial corpus에 대한 추가 pre-training의 효과.</li>\n<li>성능 감소없이 학습시 forgetting을 막기위한 것과 학습시간 단축을 위해 model의 일부 집합만 fine-tune하는 training strategy.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>논문은 다음과 같이 구성됨</p>\n<ol>\n<li>financial polarity analysis와 pre-trained language model에 관한 관련연구에 대해 discuss(Section 2.)</li>\n<li>evaluated model에 대해 설명(Section 3.)</li>\n<li>사용한 experimental setup에 대한 설명(Section 4.)</li>\n<li>financial sentiment dataset에 대한 experimental result 제시(Section 5.)</li>\n<li>다른 관점에서 FinBERT를 추가적으로 분석(Section 6.)</li>\n</ol>\n</li>\n</ul>\n<h1 id=\"2-realted-literature\" style=\"position:relative;\"><a href=\"#2-realted-literature\" aria-label=\"2 realted literature permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Realted Literature</h1>\n<ul>\n<li>본 섹션에서는 finance sentiment analysis와 language model을 이용한 text classification에 대한 previous research에 대해 설명.</li>\n</ul>\n<h2 id=\"21-sentiment-analysis-in-finance\" style=\"position:relative;\"><a href=\"#21-sentiment-analysis-in-finance\" aria-label=\"21 sentiment analysis in finance permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.1 Sentiment analysis in finance</h2>\n<p>senmiment analysis는 사람의 감정이나 의견을 추출하는 task이며 다음과 같이 두가지로 나눌 수 있다.</p>\n<ol>\n<li>“word counting”을 기반으로 feautre를 추출하는 machine learning 방법.</li>\n<li>text가 sequence of embedding으로 representation되는 deep learning method.  </li>\n</ol>\n<p>전자는 특정 단어 sequence에서 발생하는 semantic information을 표현할 수 없는 반면 후자는 훨씬 더 많은 parameter를 배우기 때문에 “data-hungry”현상이 발생한다.</p>\n<p>financial sentiment analysis는 domain뿐만 아니라 목적도 일반적인 sentiment analysis와 다르고 financial sentiment analysis의 목적은 일반적으로 시장이 text에 제시된 정보와 어떻게 반응하는지 추측하는 것이다.  </p>\n<p>Loughran and McDonald(2016)은 “back-of-words”접근법 또는 어휘 기반의 방법을 based로한 machine learning을 사용하여 financial text analysis에 관해 연구를 진행.\ntextural polarity analysis에 deep learning을 사용한 첫번째 논문에서는 LSTM neural network를 사용하여 주식 시장의 움직임을 예측하고 기존의 machine learning보다 더 정확한 성능을 보여줌.\n위의 연구는 결과를 향상시키기 위해 더 큰 corpus에서 pre-train을 하지만 pre-train은 lable이 없는 dataset에서 수행됨.  </p>\n<p>neural architecture를 사용하는 financial sentiment analysis는 Sohangir et al.(2018), Lutz et al.(2018), Maia et al.(2018)등이 있다.  </p>\n<p>Sohangir et al.(2018)은 여러가지 일반적인 neural architecture를 StockTwits dataset에 적용하여 CNN이 neural architecture중 가장 성능이 좋다고 인식.  </p>\n<p>Lutz et al. (2018)은 doc2vec을 사용하여 특정 회사 발표에서 sentence embedding을 수행하고 multi-instance learning을 활용하여 주식 시장 결과를 예측하는 접근 방식을 취함.  </p>\n<p>Maia et al.(2018)은 text simplification과 LSTM network 조합을 사용하여 financial news의 sentence set를 semantic에 따라 classification하고 FinancialPhraseBank에 대해 SotA를 달성</p>\n<p>labled financial dataset이 많지 않아 neural network를 sentiment analysis를 위해 사용하기는 어렵다.\nembedding layer에 해당하는 pre-trained된 값으로 init하더라도(e.g. Glove)나머지 layer는 상대적으로 적은 양의 data로 학습해야 함.\n가능성이 있는 solution은 model을 pre-train하여 전체를 init하고 classification task와 관련하여 해당 값을 fine-tune하는것.</p>\n<h2 id=\"22-text-classification-using-pre-trained-language-models\" style=\"position:relative;\"><a href=\"#22-text-classification-using-pre-trained-language-models\" aria-label=\"22 text classification using pre trained language models permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2.2 Text classification using pre-trained language models</h2>\n<p>최근 NLP에서 가장 중요한 발전 중 하나는 language modeling을 위해 pre-trained model을 약간만 수정하여 대부분의 downstream NLP task에 맞게 fine-tune할 수 있다는 사실이다.\n이 model들은 very large corpora에 대해 train된 다음 target dataset에서 fine-tune한다.  </p>\n<p>ELMo(Embeddings from Language Models)는 이 접근법의 첫번째 성공적인 적용 중 하나이다.\nELMo는 large corpus로부터 pre-train된 deep bidirectional language model이다.\nELMo는 contexualizing representation을 위해 pre-trained language model을 사용하지만 여전히 language model을 사용하여 추출된 정보는 모든 모델의 첫번째 layer에만 존재.  </p>\n<p>ULMFit(Universal Language Model Fine-tuning)은 차별적인 fine-tune, slanted triangular learning rate, gradual unfreezing을 사용하여 NLP에 대한 진정한 fine-tuning을 달성한 최초의 연구.\nULMFit의 주요 아이디어는 BERT(Bidirectional Encoder Representation from Transformers)를 통해 한단계 더 향상되었다.  </p>\n<p>BERT가 이전과 다른 두가지는 다음과 같다.</p>\n<ol>\n<li>language modeling task는 두개의 문장을 서로 뒤따르는 것을 classification하는것 이외에도 다음 token이 아닌 순서대로 무작위 masked token을 예측하는것으로 정의</li>\n<li>large corpus에 대해 very big network에서 학습된 전례없는 사례.</li>\n</ol>\n<p>이와같은 두가지를 통해 여러 NLP task에서 SotA를 달성.  </p>\n<p>text classification에 대한 fine-tuning BERT는 연구가 활발하진 않음.\n이러한 연구 중 하나는 Sun et al.(2019)<a href=\"https://arxiv.org/pdf/1905.05583v1.pdf%20http://arxiv.org/abs/1905.05583.pdf\">(How to Fine-Tune BERT for Text Classification?)</a></p>\n<h1 id=\"3-method\" style=\"position:relative;\"><a href=\"#3-method\" aria-label=\"3 method permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Method</h1>\n<ul>\n<li>이 section에서는 financial domain을 위해 구현된 FinBERT에 대해 소개.</li>\n</ul>\n<h2 id=\"31-preliminaries\" style=\"position:relative;\"><a href=\"#31-preliminaries\" aria-label=\"31 preliminaries permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.1 Preliminaries</h2>\n<ul>\n<li>3.1.1 ~ 3.1.5 LSTM, ELMo, ULMFit, Transformer, BERT에 대한 설명이며 생략.</li>\n</ul>\n<h2 id=\"32-bert-for-financial-domain-finbert\" style=\"position:relative;\"><a href=\"#32-bert-for-financial-domain-finbert\" aria-label=\"32 bert for financial domain finbert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.2 BERT for financial domain: FinBERT</h2>\n<ul>\n<li>3.2의 subsection에서는 <strong>1)</strong> domain corpus에 대한 further pre-training이 수행되는 방식. <strong>2-3)</strong> classification, regression task를 위해 BERT를 구현한 방법. <strong>4)</strong> fine-tuning중 사용된 training strategy들을 설명</li>\n</ul>\n<h3 id=\"321-further-pre-training\" style=\"position:relative;\"><a href=\"#321-further-pre-training\" aria-label=\"321 further pre training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.2.1. Further pre-training</h3>\n<ul>\n<li>Howard and Ruder(2018)는 target domain corpus에서 language model을 추가로 pre-train하면 최종 classification 성능이 향상됨을 보여줌.</li>\n<li>이러한 adaptation이 도움이 되는지 관찰하기 위해 further pre-training을 구현.</li>\n<li>\n<p>further pre-training을 위해 두가지 접근방식을 실험.</p>\n<ol>\n<li>target domain의 large corpus에서 model을 pre-training. 이를 위해 financial corpus에 대한 BERT language model을 pre-training함.(corpus에 대한 자세한 내용은 section 4.2.1 참조)</li>\n<li>classification dataset에서 sentence에 대해서만 model을 pre-training. 이 방식은 train data가 훨씬 적지만 직접 target의 dataset 사용하면 더 나은 domain adaptation을 제공할 수 있음.</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"322-finbert-for-text-classification\" style=\"position:relative;\"><a href=\"#322-finbert-for-text-classification\" aria-label=\"322 finbert for text classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.2.2 FinBERT for text classification</h3>\n<ul>\n<li>sentiment classification은 <strong>[CLS]</strong> token의 last hidden state다음에 dense layer를 추가하여 수행됨.</li>\n<li>이후 classifier network는 labled sentiment dataset에 대해 학습되며 모든 단계의 개요는 그림 1과 같음.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.94594594594595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZUlEQVQoz22Ri26DMAxF+f8P3KZVHdBCeSVAeJXwyJ3tQKdKi2TJHOxrJzcAnWmaMAwDp3g+n5SPr7zve8mttVLjnMMwjpjnGf+dgP6jNyUa/cC6UnGn0ao79h0Ye008Fd4z1wk24kOn0NYP2GWVQRzLssiwYKdO3qTrDIwxGGn6OA4UkxR15o/zhnwbFjAHZ+Z7Ri/ITW3bIs9zCa1rVJVC0zTSnGXZwTXxCg3VcvPj5ErLv7quwcsFPK0oCoRhiPv9BlWVSNNUGL9fFEVI0gRKKyRJIqLGtLher4jjmOpy4SwugvyQ27ZhXSxK3eNy13JVDubLuqDKNOLv5I07t8PUPW6XVIQ8c2yKgzscyknwK9JkwvbmXJkqhJ83aTwPC+qixs9HDDtbETtcdkeBoy1p+rq8HOM4t9y21fPdc7+tpSGbmHQO8xuKphPX2Aw2iR9ZK+WNIMbfSnljFPGajGD+vrXDL8d0tsAywEJ6AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig1\"\n        title=\"fig1\"\n        src=\"/static/18b6d77f05187869a3504bc87460a485/fcda8/fig1.png\"\n        srcset=\"/static/18b6d77f05187869a3504bc87460a485/12f09/fig1.png 148w,\n/static/18b6d77f05187869a3504bc87460a485/e4a3f/fig1.png 295w,\n/static/18b6d77f05187869a3504bc87460a485/fcda8/fig1.png 590w,\n/static/18b6d77f05187869a3504bc87460a485/efc66/fig1.png 885w,\n/static/18b6d77f05187869a3504bc87460a485/c83ae/fig1.png 1180w,\n/static/18b6d77f05187869a3504bc87460a485/2a08f/fig1.png 1422w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<h3 id=\"323-finbert-for-regression\" style=\"position:relative;\"><a href=\"#323-finbert-for-regression\" aria-label=\"323 finbert for regression permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.2.3 FinBERT for regression</h3>\n<ul>\n<li>본 논문에서는 classification에 중점을 두지만 연속 대상이 있는 다른 dataset에서 거의 동일한 architecture로 regression을 구현</li>\n<li>유일한 차이점은 사용되는 loss function이 MSE(Mean Squared Error)라는것.\n</li>\n</ul>\n<h3 id=\"324-traning-strategies-to-prevent-catastrophic-forgetting-치명적인-forgetting을-방지하기-위한-training-strategy\" style=\"position:relative;\"><a href=\"#324-traning-strategies-to-prevent-catastrophic-forgetting-%EC%B9%98%EB%AA%85%EC%A0%81%EC%9D%B8-forgetting%EC%9D%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%9C-training-strategy\" aria-label=\"324 traning strategies to prevent catastrophic forgetting 치명적인 forgetting을 방지하기 위한 training strategy permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3.2.4 Traning strategies to prevent catastrophic forgetting (치명적인 forgetting을 방지하기 위한 training strategy)</h3>\n<ul>\n<li>Howard and Ruder(2018)에 의해 지적된바와 같이, catastrophic forgetting은 fine-tuning 접근법에서 중대한 위험이다.</li>\n<li>fine-tuning을 진행중에 model이 새로운 task에 adaptation하려고 시도할 때 language modeling task에서 정보를 “forget”할 수 있다.</li>\n<li>이러한 현상을 처리하기 위해 Howard and Ruder(2018)가 제안한 3가지 기술인 <strong>slanted triangular learning rate, discriminative fine-tuning, gradual unfreezing</strong>을 적용한다.</li>\n<li>Slanted triangular learning rate는 기울어진 삼각형 모양으로 learning rate를 적용한다. 즉, 학습속도는 어느 시점까지 linear하게 증가하고 그 이후에는 linear하게 감소.</li>\n<li>Discriminative fine-tuning은 networ의 하위 layer에 대해 낮은 learning rate를 사용.</li>\n<li>layer <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>에서 learning rate가 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span></span></span>라고 가정.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>에 대해 layer <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l-1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>의 learning rate를 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>α</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mrow><mi>θ</mi><mi>α</mi></mrow><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">{\\alpha}_{l-1}={\\theta \\alpha}_{l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.638891em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord mathdefault\" style=\"margin-right:0.0037em;\">α</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 로 계산.</li>\n<li>이 방법은 하위 layer에는 deep-level language information, 상위 layer에는 actial classification task에 대한 information 이 있다는 가정이 있음.</li>\n<li>Gradual freezing을 통해 classifier layer를 제외한 모든 layer에 대해 고정된 training을 수행.</li>\n<li>training을 하는동안 가장 높은 layer부터 시작하여 모든 layer를 점차 unfreeze하여 lower level feature가 가장 미세하게 조정됨.</li>\n<li>따라서, training의 초기단계에서, 모델이 pre-training으로부터 배운 low-level language를 “forget”하는것이 방지됨.    </li>\n</ul>\n<h1 id=\"4-experimantal-setup\" style=\"position:relative;\"><a href=\"#4-experimantal-setup\" aria-label=\"4 experimantal setup permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Experimantal Setup</h1>\n<h2 id=\"41-research-questions\" style=\"position:relative;\"><a href=\"#41-research-questions\" aria-label=\"41 research questions permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4.1 Research Questions</h2>\n<ul>\n<li>\n<p>우리는 다음과 같은 research questions들에 대해 답하고자 함.</p>\n<ol>\n<li>ELMo 및 ULMFit과 같은 다른 fine-tune 방법과 비교하여 short sentence classification에서 FinBERT의 성능은 어떤지?</li>\n<li>FinBERT는 target이 discrete하거나 continuous한 financial sentiment analysis task에서 SotA와 어떻게 비교되는지?</li>\n<li>financial domain 또는 target corpus에서 further pre-training BERT는 classification performance에 어떤 영향을 주는지?</li>\n<li>triangular learning rates, discriminative fine-tuning and gradual unfreezing이 classification performance에 미치는 영향이 무엇인지? 이를 통해 catastrophic forgetting을 방지할 수 있는지?</li>\n<li>sentence classification에 가장 적합한(또는 더 안좋은)encoder layer는 무엇인지?</li>\n<li>fine-tuning만으로 충분한지? 즉, pre-train이후 전체 model을 fine-tune하는데 얼마나 많은 layer들은 fine-tune해야 하는지.</li>\n</ol>\n</li>\n</ul>\n<h2 id=\"42-datasets\" style=\"position:relative;\"><a href=\"#42-datasets\" aria-label=\"42 datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4.2 Datasets</h2>\n<ul>\n<li>\n<p><strong>TRC2-finalcial</strong></p>\n<ul>\n<li>BERT를 further pre-train하기위해 TRC2-financial이라고 하는 재정적인 자료를 사용.</li>\n<li>2008년~2010년 사이에 Reuters에서 발행한 1.8M 뉴스 기사로 구성된 Reuters TRC2의 subset</li>\n<li>TRC2-financial은 29M이상의 단어와 400K문장을 가진 46,143개의 document를 포함.</li>\n</ul>\n</li>\n<li>\n<p><strong>Financial PhraseBank</strong></p>\n<ul>\n<li>본 논문에 사용된 main sentiment analysis dataset은 Financial PhraseBank</li>\n<li>LexisNexis database의 financial news에서 무작위로 선택된 4845개의 영어 문장으로 구성.</li>\n<li>모든 문장의 20% test 20% validation으로 설정하며 train에는 3101 example이 포함됨.</li>\n<li>일부 실험에는 10-fold cross validation을 사용.</li>\n</ul>\n</li>\n<li>\n<p><strong>FiQA Sentiment</strong></p>\n<ul>\n<li>FiQA는 WWW’18 financial opinion mining과 QA challenge를 위해 만들어진 dataset.</li>\n<li>Financial Phrasebank와 달리 이 dataset의 목표는 [-1. 1]사이의 contunuous한 값이며 1이 가장 긍정적이다.</li>\n<li>10-fold cross validation을 사용하여 검증</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"43-baseline-methods\" style=\"position:relative;\"><a href=\"#43-baseline-methods\" aria-label=\"43 baseline methods permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4.3 Baseline Methods</h2>\n<ul>\n<li>contrastive experiment에서 GLoVe embedding을 포함한 LSTM classifier, ELMo embbeding을 포함한 LSTM classifier, ULMFit classifier를 사용.</li>\n</ul>\n<h2 id=\"44-evaluation-metrics\" style=\"position:relative;\"><a href=\"#44-evaluation-metrics\" aria-label=\"44 evaluation metrics permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4.4 Evaluation Metrics</h2>\n<ul>\n<li>classification model을 검증하기 위해, Accuracy, Cross entropy loss, F1 average를 사용.</li>\n</ul>\n<h1 id=\"5-experimental-resultsrq1-rq2\" style=\"position:relative;\"><a href=\"#5-experimental-resultsrq1-rq2\" aria-label=\"5 experimental resultsrq1 rq2 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Experimental Results(RQ1, RQ2)</h1>\n<ul>\n<li>Financial PhraseBank dataset classification task에 대한 FinBERT의 결과는 표2에서 확인할 수 있음.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.83783783783784%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+0lEQVQ4y01TiZKqQAz0///Ot64oqCCHAgPIoSiX2XQDVS9VUxmSTKdzsPl+vxLHseR5IVlmJE0SSZJU0jSV+/2u9lyMMfzOTMYY2BONg4Y9VX8/DALZ9H1HY14Us1MD+TjLqAu1AxRntcOGJAbvFt/n85kBcem6TlaN836/+Y3Hp5NDANjgAyvP8+R6vUoQBOK6rsacWBUBEeyrA6VDpnFkOciKB77vE9j3A9oSbQ+Ywg5Q+AAKGwFvt5scjkd5PB5k0jQNgZrnk+WjFeM4EOCptjAMqZE0jhP1jRJFodR1PQMig+M4pIz76/Vitq7vpSwfUlYVA9EzPI7jO8uvqpLJVl/fL0NBCUEQsme9gkCDadu++djoZDu1gVmt7MuyZFJjUkk0MeKDwNcEC0MERlo2ZO3jKmAIYAhXQxOugvUxWc67pz1s23YGRKP3e4sNB+Aw9Jqt4kTBBGfQHUOPYGuaWRdcl0KmaSJ7xBEwiiLZ7XZsMkoCmOe5LPuq6zEPoZGjDg5J4cOPsK5No4nmGSQzoG0fZbvdKsu9nl85HA5i2za1ZVkE+vn5R40Y7Bz8WJU1xrL29IPtxtdMp/OZfUDwWe9gBg375XLRA+3yDp9tO1xuxIMllh2sp+krG+wUJoeDMjMtCyuB8ldfs0wX7UAv0bta79Wye//LHwwX05qCwUB2AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table2\"\n        title=\"table2\"\n        src=\"/static/ee933c267a12e458d384654d2f15a071/fcda8/table2.png\"\n        srcset=\"/static/ee933c267a12e458d384654d2f15a071/12f09/table2.png 148w,\n/static/ee933c267a12e458d384654d2f15a071/e4a3f/table2.png 295w,\n/static/ee933c267a12e458d384654d2f15a071/fcda8/table2.png 590w,\n/static/ee933c267a12e458d384654d2f15a071/c3fd4/table2.png 854w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n<li>FinBERT는 가장 높은 성능을 보여주며, language model information이 없는 LSTM classifier는 가장 성능이 떨어짐.  </li>\n<li>FiQA sentiment dataset에 대한 결과는 표3과 같다.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 559px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACLElEQVQ4y5VTabOaQBD0//8wK08rwQMRkJsFFTzwBjwm04OQvOR9SLZqrFl2tqe7Z+0Rr+fzSa/Xi9r8q/XVd9xp77Wrh580TWkwGFAQBJLr0ylNOUYjjcaTCRnzOY20H7wf02Q8piiOyTAM2c/5DPX6bEZ1XTeAURjQx8c30rhgOBxSv98nn8Hv9ztdr1eqqlry2+1GdVXxvgnZM0hVllSWlbDtNXIeHXUUIP6U8i+rAzydTrRYLCQ22y0dDgXt93uJ3W73VxRFwfb4LNfk2gN5nkfr9foXIGS5jsOADh2PR5FyPp8lkCNQ00ZVlZSomEzTkjP4XjBwN5Tb7UoOs3O5k2LD0dG2bTG8ZH/gFzxEwA5MPM8zcpgEzjHMJEk/A0JuFEXkuh7NZjqH0QGj0fP56kAhbcfWhGEoDQC2Y3s6QMjARTwVAEM2fG1lI1q5l8tFGIf8Miyz8dCyLCHTAQJgpuukc6ArjEdhO5jinbcDQTPItBkI30DmE+CSTf2uabTgA89zyeRCk/2D6a7rUsy+zucGBWFEKlEC4MgQFzJp3w/EHtghgHmeS9FyuaLVakWB75PnOvyPUM2QGBQ+xu+BYW9ZJimleJi2AIbMuK7fgNvNRuirJGnos2zkURSLryZ7JRJtS0DBGtbgLUIRmjjubwzTNJH/a8yMUAizURSyxISBsywTNni8WbamJatAHQLf0CTLcro/Hg1gwYYrlfBhRhtmi8tghPx/10/Hx2f0BGAnWAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table3\"\n        title=\"table3\"\n        src=\"/static/cef7da8313541b52ef72858812120b8e/a65ce/table3.png\"\n        srcset=\"/static/cef7da8313541b52ef72858812120b8e/12f09/table3.png 148w,\n/static/cef7da8313541b52ef72858812120b8e/e4a3f/table3.png 295w,\n/static/cef7da8313541b52ef72858812120b8e/a65ce/table3.png 559w\"\n        sizes=\"(max-width: 559px) 100vw, 559px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n<li>FinBERT는 좋은 성능을 보여줌.</li>\n<li>testset은 사용할 수 없으므로 10-fold cross validation으로 검증.</li>\n</ul>\n<h1 id=\"6-experimental-analysis\" style=\"position:relative;\"><a href=\"#6-experimental-analysis\" aria-label=\"6 experimental analysis permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. Experimental Analysis</h1>\n<h2 id=\"61-effects-of-further-pre-training-rq3\" style=\"position:relative;\"><a href=\"#61-effects-of-further-pre-training-rq3\" aria-label=\"61 effects of further pre training rq3 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6.1 Effects of further pre-training (RQ3)</h2>\n<ul>\n<li>classifier performance에 대한 further pre-train의 효과를 측정.</li>\n<li>\n<p>비교는 세가지 모델을 사용:</p>\n<ol>\n<li>No further pre-traing</li>\n<li>Further pre-training on classification training dataset(FinBERT-domain으로 표시)</li>\n<li>Further pre-training on domain corpus, TRC-financial(FinBERT-domain으로 표시)\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 527px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 58.10810810810811%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACF0lEQVQoz5VT147iQBDk//8J3b7cSnewmORMcE4kY5MFW9fVu7DPN1LLnlRdXV3TicIQlm2jaRocj0esVit4nocwjCQCzGY+srzAvt5hOBphvV7Dti30Pz7g+zMURYEsyxEIDkfHnIzx6+0Ng8EAy+USo/EYvV4Pv9/fZc1AmqZYCcihbRFFsSYmSBzHiJMEdV2jqirkeY7Pz0901uuVZMgQCFgoWdI0k80Mrutiv2/wv6NzuVzAuF6vWvLpdNK4nM84HA4CuldWzy+jbRtlttvtXuutVKAMiyLH0DBgSMn7psX5G+h52RZ9yZ7aTScTuI6D6dSU+Ua19n0fE1mnDMqwFrGDYIn5fKGlU8eyLJUlGcdRhER05H9VlXJujlwuP5MmoiP3OVdA0vc9FxPJalmWZi+KUjdZQpLE2hiO6+UsjnBwu91emjlSQf7NTkuuRQfPdcQGBhzH1ozLIHgdiONImXOcT0eYlq2ycDzud5FrIBWGP4CbDbUZ48/fnmpE8OFwKIynquloaGAhMtzvDxTS/W63K5JUMr9rM+nJOE7weDy+ABNhYJmmakh2kWjGjCyVB8mOnsuyFJF86TeeSyW4zvO0G9cUMFgu0O/31cSO42KxWKgHZ74H1/PlQiivZSZ7jsYzIUlMpQpTdCfYdrv9AmSn2GlmLsXx9BefXy7MON9uN+o1Nord56uh/7hP9rSL4Lw0/Ae/U3wwBQm6sAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table4\"\n        title=\"table4\"\n        src=\"/static/500332c5afc0c22828683a82b8cd44d6/44385/table4.png\"\n        srcset=\"/static/500332c5afc0c22828683a82b8cd44d6/12f09/table4.png 148w,\n/static/500332c5afc0c22828683a82b8cd44d6/e4a3f/table4.png 295w,\n/static/500332c5afc0c22828683a82b8cd44d6/44385/table4.png 527w\"\n        sizes=\"(max-width: 527px) 100vw, 527px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n</ol>\n</li>\n<li>차이는 크지 않지만 financial domain corpus에서 pre-training을 진행한 classifier는 세 가지 중에서 가장 잘 수행됨.</li>\n<li>\n<p>4가지 이유라고 가정.</p>\n<ol>\n<li>corpus는 task set과 다른 distribution을 가질 수 있음.</li>\n<li>BERT classifier는 further pre-training을 통해 크게 향상되지 않을 수 있음.</li>\n<li>short sentence classification은 further pre-training이 큰 도움이 되지 않을 수 있음.</li>\n<li>성능이 이미 우수하여 개선의 여지가 많이 없음.</li>\n</ol>\n</li>\n<li>가능성은 마지막이 젤 크다고 추측. 이미 Vanilla BERT의 정확도는 0.96임.</li>\n<li>domain corpus에 대한 further pre-training의 효과가 중요하지 않다는 결론을 내리려면 다른 financial labled dataset에 대한 실험이 필요.</li>\n</ul>\n<h2 id=\"62-catastrophic-forgetting-rq4\" style=\"position:relative;\"><a href=\"#62-catastrophic-forgetting-rq4\" aria-label=\"62 catastrophic forgetting rq4 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6.2 Catastrophic forgetting (RQ4)</h2>\n<ul>\n<li>\n<p>Catastrophic forgetting에 대한 techniques를 측정하기 위해 4가지 설정을 진행:</p>\n<ol>\n<li>No adjustment(NA)</li>\n<li>only with slanted triangular learning rate(STL)</li>\n<li>slanted triangular learning rate and gradual unfreezing(STL+GU)</li>\n<li>discrimanative fine-tuning + 이전</li>\n</ol>\n</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.02702702702703%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAACgElEQVQ4y6VT2W7bMBD0/39PH/uQvBUI2gBpYsvWZR0WL0nWLUs+pkMlaYGkQB8qYEViSc7uzs6ubrcbrtcrLpcLpumMk7XTzP38tp4x06Y3/2LTlet5uTMMI+b5jNvtCou1sj8L1o8jjJ9BRgZS1ZCy4togiwTiXYDvP17gOCGUbJCKZ+zcNZ6ePGw2AY7HFoBN7A3QWn86Id8LqG0CZVooVUHpGllSIvIUH2/h+ynSNEeuMyTxAZ534L0aTTPgHWdFaEY4om1bGFNDrePlkibYsooK+nBcgJJEI8sKBhp53vD+kZWUnwHjOKazRV52kC6zjHNm2Sxl2yytaYIb+pTuGSRBnq1hsg10tkVTd6+A7MVvwK7rkRctZJJD+voPoC1d2T3PVEsAF7lwmKH1twvfnzJ0XRd1XSPP2+WSchXJt+UwI9XBqBxGxCjlBoUiJXqAYAAhyHFGwPoDoAXr+wHSGER8IHwBHZG7oiTYGlI84cDM9kmCXaSwCyKsdw5eXAeb0EfFknH70GWrJykJGK8R7L/h8PwV0eMXOI93cF5+UiY7OL4H391i7/oMeiDfEeQuQlP1FI3l8F2H5zM6Zmh0TpnsEYZ7+EGI0JMQHstlo8p1iPLZQ74NUfgRJRbD0LQf/wWQwp7nmaAjJ+CCkVPQNJyA86vgazas7XtSU2EcBgxdh5aqmOYL/aelOjtt1/cu/+uzHAshLE3//FYnToh9MDITG11KSUlolGW5mBV9URSL2X1VWW3qJYBhE62v5Jkx+YKxMlrh7v4eDw8PHC0fYRAgoKVpSklki1l/zA7bvQ0WhiHHzsM+ihZfyjOfb2yA1TRN1F++ADRNg//9fgGd2XGMkmXskgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig3\"\n        title=\"fig3\"\n        src=\"/static/878f3bbfcbe06b7c5c5760292bbd01e2/fcda8/fig3.png\"\n        srcset=\"/static/878f3bbfcbe06b7c5c5760292bbd01e2/12f09/fig3.png 148w,\n/static/878f3bbfcbe06b7c5c5760292bbd01e2/e4a3f/fig3.png 295w,\n/static/878f3bbfcbe06b7c5c5760292bbd01e2/fcda8/fig3.png 590w,\n/static/878f3bbfcbe06b7c5c5760292bbd01e2/295bd/fig3.png 667w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 466px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.2972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC60lEQVQ4y6VUaW/aUBDM//9TqdRUaVOVKxjwhW9sCNjmtA1JtjNrjMLHqk+y/M55uzOz70HQNpuNWJYl6WKh/TAMJY5jSdNUlstMsizTea77QaDrhmFIivklvgBzURRJ0zTyQMAYg6enHzIajcR15zLo92UwGMpwOJQ++uPxWDzfFxuXjgHE8cvLi5imKbPpRCbTqQxx9ng6tYCu40iv15Pn52cFeX19BdBAfjx9x+aZ/EtTwM/PTx18fHzI5XLR8fv7u5zPZ+3zz/mvH9Pr+t1enlfAosjFth1wE0mSJOBtKeu3N+WRc6vVSufJU4Q5zr9h3QcN5I8f1+u6bgGbphZzNgN/rvxB6tvtViNeAdidz+VwPEKYFBzaMsfYBJeHwwGiLVQU27ZVrFvK+91WwXhzWZay3+/1ACNzwG9RlFJVlR72PE/3cP2Ii+I4QoShjpm2Al7ODZQzNJqvLc83ao+qbq4RZ+LABUytawkAXc+/F4Up0zIFbqYIJ8iv3Oa5UnGq6hsgeauuY+6NwkBsXEJB7gBnONiR73ltpPlmLVN47HiqdMyUDWOCiOubK0aDnvz89VvOUPsOkAfJFdUbwYvkZL/bKYd106Zcwg1xslA+NUIAUrgoTtQ6N0B6iQdZCVQxz4sbPzR5F6GHNVZON2ZbJDGqKLjnsCwKVZmlxC/LlrJDdAH4Yo2vUccc87LhYCAZotqCb6odoa4JmIPvL7UcyuPjN2zuizGZKjjr1bYtgHpiwfQBDrFEffwdx9Y1HxZi7fNSalAgsIcu7DlAGIHjuJrqzGw3cY3VQXVpYM9jdfgaJYFnU9rNV8vRvwpIxxuIyAQIVQ7henfuASxRbhmV6zoKqCWKdYpDyzCbBM/aHNEeDsdrhJiwsJmLNLJ7jZYiUf0WyJaePmWGWFeuucbnzFGKDNkxQppziToNNCVHH1Vahw8E++t1+0hsoDwjKMtC65oPb5pm6l26gvub5lp6OVQk4AIb/rf9BfFSjDJ8+r3qAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table5\"\n        title=\"table5\"\n        src=\"/static/0f45c1d3e02d6f10dcadede394bca5f3/fc1a1/table5.png\"\n        srcset=\"/static/0f45c1d3e02d6f10dcadede394bca5f3/12f09/table5.png 148w,\n/static/0f45c1d3e02d6f10dcadede394bca5f3/e4a3f/table5.png 295w,\n/static/0f45c1d3e02d6f10dcadede394bca5f3/fc1a1/table5.png 466w\"\n        sizes=\"(max-width: 466px) 100vw, 466px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<ul>\n<li>language modeling에서 배운 정보는 대부분 low-level layer에 존재하기 때문에 STL+DFT만 사용하면 STL만 사용한것보다 성능이 저하된다는 것을 알 수 있다.</li>\n<li>gradual unfreezing이 가장 중요한 기술임을 보여줌.</li>\n<li>NA의 경우 심각하게 overfitting이 되며 GU를 사용하여 이를 방지할 수 있음.</li>\n</ul>\n<h2 id=\"65-where-does-the-model-fail\" style=\"position:relative;\"><a href=\"#65-where-does-the-model-fail\" aria-label=\"65 where does the model fail permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6.5 Where does the model fail?</h2>\n<ul>\n<li>model이 예측하지 못한 몇가지 example에 대하여 조사.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 531px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.48648648648649%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAABn0lEQVQoz32TiY7CMAxE8/8/VglUBOI+yn2WFig3LeDVGykIrbSLlMZOHHs8Y9zr9bL5fG79ft+iKLI4jm29Xtt2u7X9fi8/yzLbbDaKS5JEizvOsZfLpe12O3u/3+bSNLVyuWxhGFqz2VQyX2A6ndpsNrPBYGCLxcLG47HsVqulGGKJ47zT6aiA40GtVtNjUHW7XT0oisJut5s9Ho/Pnuf5Z6cz1vP5FDJ2fMeHZKPRSFVAMJlM5HNO8kajIZ87Cp7PZ/vr5/gAldZZl8tF/IDkeDyKm8PhYKfT6YMUG5TEckeB+/2urhwHICuVShYEgbhBCKiAU4QCMTboEIE7bFa73bbhcKgcCOdWq5UecUAyyOcRPg9BSAGSsjhDBGJBBkoW/OM7WqjX60LAwq5UKhKqWq1qAvC5Q1EQQc2/HF6vV/EFd+z4VKWi5xCbOy/It7LfajvIpbJHRMu0RZsoiw0FtESbvm2UZwoYanx4JE5zyFB7oknCqHiysTmnGK2itJ9Dj/QbsVr2/4Jer6eEoGDmEIyEXm2KEkPrv5N5+wdGhH7EXXa7cwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig_t.png\"\n        title=\"fig_t.png\"\n        src=\"/static/8753fb44696e3b63d19bff136777c7d1/d4713/fig_t.png\"\n        srcset=\"/static/8753fb44696e3b63d19bff136777c7d1/12f09/fig_t.png 148w,\n/static/8753fb44696e3b63d19bff136777c7d1/e4a3f/fig_t.png 295w,\n/static/8753fb44696e3b63d19bff136777c7d1/d4713/fig_t.png 531w\"\n        sizes=\"(max-width: 531px) 100vw, 531px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></li>\n<li>첫번째 예는 가장 일반적인 실패 유형이다. model은 수학을 수행하지 못하고 “increased”와 같은 방향성을 나타내는 단어가 없으면 neutral을 예측할 수 있다.</li>\n<li>두번째, 세번째 예는 동일한 유형의 실패 유형이다. model은 주어진 상황에 대한 중립적 진술을 회사에 대한 polarity를 나타내는 진술과 구별하지 못함.</li>\n<li>세번째 예에서는 회사의 비지니스에 대한 정보가 도움이 될것이다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 407px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 112.16216216216218%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAWCAYAAADAQbwGAAAACXBIWXMAAAsTAAALEwEAmpwYAAACrUlEQVQ4y42VB2/jMAyF8/9/UppLk9w5e6FNs5C9h7PaZg+eP/bkKjkccAQMWxRFvvcoyQH5Y7fbzX/zXK9XHS+XS4lEIvL8/CyhUEiCwaAUCgWdI8bEm7UBO9njN3a5XGSxWMhsNtPkk8lE1uu1PJpJHjAVTCIS1Go1yeVykkwmZbPZqC+dTiuyYrGovvF4rL58Pi+NRuMboQ0XW61W0m63NWm5XJZ+vy+DwUBeX191caVSkWazKb1eT7+r1aq0Wi2Zz+dyPp/vKQ+HQ6nX67Lb7XzUfIMAlMaIm06nd0C63a58fn5+U8ZisZhWxKDEg6EfurEIHd/f33UxbDqdjr6Z/ythJpNRGlipVJK3tzf9Bg20U6mUvtHw4+NDkzmOo5TvEhroJCPIpkf3SGIbej12mqLb7fZLQwQnCENwNILuaDTSwJeXFx3jhz4NI44526cISRIOh3Ux3aUptoGcRba5rqu0bSOpjzCRSKiw7DuawgSo2DaGDvNIAlLo0hikIA52NMtHmM1mNSAej+vmPZ1OupiuGs2YR3yQ8bCYQlClQRT0E7LjoUyX6ezhcFCqNGe/32ui4/GoY5pELBRNjGmcT5lDb3RCR9MMzi0LaZoZc67RmSJoiY83Ph8hJwEEUCTw0cypMAZl0NiGhorQvl2gTiUWsIm5qqgKSpCjMefZnBa2TjQa1TW+hkYHzHQZTUgCHeb55kwzNscMNDzIw9xdQoOSStCnazSGhyB8LKZB7ABuILYP8/iQi9vHb4pNm2oIDy2e1WqpYxAYn+su7nyuO9fGwNTX8Hw+KTUqmqpf3x5KDzGa6dhDBUpF7/m3291dc/yEu91WQk9PEv4Rkqx3W7Nh2dxs9LD3P3GcX/LTu94SiaTeRI4Tl7jnS6YycvCK//OfYt80/2v2+t9Wqn7jDM7WxQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fig4\"\n        title=\"fig4\"\n        src=\"/static/f9fc6d8b570977c2f4540039b0a233e1/0ff56/fig4.png\"\n        srcset=\"/static/f9fc6d8b570977c2f4540039b0a233e1/12f09/fig4.png 148w,\n/static/f9fc6d8b570977c2f4540039b0a233e1/e4a3f/fig4.png 295w,\n/static/f9fc6d8b570977c2f4540039b0a233e1/0ff56/fig4.png 407w\"\n        sizes=\"(max-width: 407px) 100vw, 407px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span></p>\n<ul>\n<li>confusion matrix는 그림 4와 같다.</li>\n</ul>\n<h2 id=\"7-conclusion-and-future-work\" style=\"position:relative;\"><a href=\"#7-conclusion-and-future-work\" aria-label=\"7 conclusion and future work permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>7. Conclusion and Future Work</h2>\n<ul>\n<li>본 논문에서는 financial sentiment analysis를 위해 FinBERT를 제안.</li>\n<li>BERT를 financial domain에 최초로 적용했으며 사용한 dataset에서 SotA를 달성하였고 classification task의 경우 정확도가 15% 향상됨.</li>\n<li>또 다른 확장은 FinBERT를 사용하여 financial domain에서 entity recognition이나 QA같은 다른 NLP task에 사용될 수 있다는것.</li>\n</ul>","frontmatter":{"title":"[논문리뷰] FinBERT: Financial Sentiment Analysis with Pre-trained Language Models","date":"September 04, 2019"}}},"pageContext":{"slug":"/NLP/finbert/","previous":{"fields":{"slug":"/NLP/albert/"},"frontmatter":{"title":"[논문리뷰] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","category":"NLP","draft":false}},"next":{"fields":{"slug":"/NLP/encyclo/"},"frontmatter":{"title":"[논문리뷰] Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model","category":"NLP","draft":false}}}},"staticQueryHashes":["3128451518","96099027"]}