{"componentChunkName":"component---src-templates-blog-post-js","path":"/NLP/rlforlm/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","author":"[Jeonsworld]","siteUrl":"https:jeonsworld.github.io","comment":{"disqusShortName":"","utterances":"jeonsworld/blog-comment"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"c6af10bf-0efd-5d14-a705-29531f982c9f","excerpt":"ChatGPT와 후속 LLM(Large Language Model)이 등장하면서 “RLHF” 라고 불리는 “인간 피드백을 통한 강화학습”의 중요성에 대한 많은 논의가 있었다.   나는 “RL이 Supervised Learning보다 더 나은 이유가 무엇일까?”, “언어모델을 지침을 통해 학습하는것(Instruction fine-tuning…","html":"<p>ChatGPT와 후속 LLM(Large Language Model)이 등장하면서 <strong>“RLHF”</strong> 라고 불리는 <strong>“인간 피드백을 통한 강화학습”</strong>의 중요성에 대한 많은 논의가 있었다.  </p>\n<p>나는 <strong>“RL이 Supervised Learning보다 더 나은 이유가 무엇일까?”, “언어모델을 지침을 통해 학습하는것(Instruction fine-tuning)만으로 충분하지 않은 이유는 무엇일까?”</strong> 라는 질문으로부터 정답을 찾기위해 노력했다.</p>\n<p>언어모델에 대해 잘모르는 사람을 위해 추가했습니다. 기본적인 내용을 알고있는 사람은 Background를 스킵해도 됩니다.  </p>\n<h2 id=\"background-supervised-learning-and-rl\" style=\"position:relative;\"><a href=\"#background-supervised-learning-and-rl\" aria-label=\"background supervised learning and rl permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Background: Supervised Learning and RL</h2>\n<p><strong>사전학습(Pre-train):</strong> 언어모델은 사전학습 단계에서 모델은 다음 토큰을 예측할 목적으로 큰 텍스트 본문에 대해 학습된다.\n사전학습 이후 모든 단어 시퀀스에 대해 잠재적인 <strong>다음 단어에 대한 확률을 할당</strong>할 수 있는 모델을 가지게된다.\n사전학습 이후 모델은 텍스트를 생성하고 주어진 텍스트 접두어(prefix)에 대한 자연스러운 출력을 생성할 수 있지만 “의사소통”에는 좋은 출력을 생성하지 못한다.<br>\n예를 들어, 질문을 입력하면 이에대한 답변을 할 수도 있고, 추가 질문을 생성할 수도 있고, 또는 질문을 의역(paraphrase)할 수도 있다.\n계속해서 문제를 해결할 수 있는 방식으로 입력 텍스트를 작성하여 모델이 원하는 동작을 수행하도록 할 수 있지만(i.e., prompt engineering) 이는 전문가가 아닌 사용자에게는 유용한 상호작용 모드가 아니다.\n언어모델에 질문이나 지침을 입력하고 모델이 이를 지속적으로 수행하도록 하기위해 우리는 언어모델을 <strong>“미세조정(fine-tuning)”</strong>한다. 이를 통해 사전학습된 언어모델이 원하는대로 작동하도록 학습할 수 있다.\nOpenAI에서는 이것을 원하는 동작에 맞춰 모델을 “정렬(alignment)“한다고 부른다.  </p>\n<p><strong>지도학습(Supervised Learning, a.k.a. Instruction Tuning):</strong> 지도학습을 위해 질문이나 지침의 형태를 갖고 원하는 출력이 포함된 인간이 작성한 여러 데이터를 수집한다.\n예를들어 <code class=\"language-text\">summarize the following text: {text} summary: {summary}</code> 와 같이 지침과 입력 그리고 요약의 결과인 출력이 존재하는 데이터 형태의 데이터셋을 학습에 사용한다.\n이를 통해 다음 토큰을 예측하는 목표에 대해 모델을 학습함으로써 지도학습 단계에서는 지침(+입력)-출력 쌍 모음에 대해 모델이 지침을 수행하여 응답하는 방법을 학습할 수 있다. 즉, 모델을 주어진 쿼리에 대한 올바른 출력이 무엇인지를 학습하고 모델은 학습 과정에서 볼수없는 다른 쿼리로 일반화하게 된다.</p>\n<p><strong>강화학습(Reinforce Learning):</strong> 강화학습에서는 모델에 지침을 제공하지만 사람이 작성한 답변은 제공하지 않는다.\n대신 모델은 자체 답변을 생성해야 한다. Scoring mechanism(인간)은 생성된 답변을 읽고 생성된 답변이 좋은지 나쁜지 여부를 알려준다.\n모델의 목표는 높은 점수를 받을 수 있는 방식으로 답변하는 방법을 배우는 것이다. 즉, 모델의 목표는 낮은 점수의 답변이 아닌 높은 점수의 답변을 생성하는 방법을 배우는 것이다.  </p>\n<p>RL은 여러가지 이유로 지도학습보다 어렵다. 이러한 어려움을 감안할때 지도학습뿐만 아니라 RL을 사용해야 하는 이유는 무엇인가?  </p>\n<h2 id=\"다양성\" style=\"position:relative;\"><a href=\"#%EB%8B%A4%EC%96%91%EC%84%B1\" aria-label=\"다양성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>다양성</h2>\n<p>언어모델 맥락에서 지도학습에 대한 직관은 모델에게 인간이 제공한 정확한 답을 복제하도록 학습하는 것이다. 반면 인간 언어의 현실은 답을 전달하는 다양한 방법이 있다는 것이다.\n인간언어에서 답변은 의미가 동일한 메세지이면서 모두 유효한 답변일 수 있다.\n그러나 모델학습시 우리는 규정된 텍스트에서 조금이라도 벗어나면 모델을 “처벌”하여 혼란을 야기할 수 있다.\n따라서 우리는 RL학습이 제공할 수 있는 다양성을 모델에 포함시키길 원한다.\n이것은 직관적인 주장이면서 잘 작동하는것으로 보인다. 그러나 RL학습은 학습이 매우 까다롭고 다양한 어려움이 존재한다.  </p>\n<h2 id=\"핵심\" style=\"position:relative;\"><a href=\"#%ED%95%B5%EC%8B%AC\" aria-label=\"핵심 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>핵심</h2>\n<p>언어모델과 상호작용에는 세 가지 방식이 있다.  </p>\n<ol>\n<li><strong>텍스트 기반:</strong> 모델에 텍스트와 지침을 제공한다. <code class=\"language-text\">주어지는 텍스트를 한국어로 번역하세요</code>와 같은 지침에 대한 답변을 생성한다.</li>\n<li><strong>지식추구:</strong> 모델에 대한 질문이나 지침을 제공하고 모델의 내부지식(“독감의 일반적인 원인은 무엇인가요?“)을 기반으로 답변을 생성한다.</li>\n<li><strong>창의성:</strong> 모델에 질문이나 지침을 제공하고 창의적인 결과를 생성한다. (“…에 대한 이야기를 써주세요.“)</li>\n</ol>\n<p>RL에 대한 주장은 상호작용 유형 중 <strong>2번에</strong> 해당된다. 즉, 진실한 답변을 기대하는 지식추구 쿼리와 “잘 모르겠습니다.”라고 말하거나 상황에서 답변을 거부하는 모델의 능력이다.  </p>\n<p>이러한 유형의 상호작용 에서는 지도학습 방식이 모델에 거짓말을 학습하기 때문에 RL 학습을 해야 한다. 핵심 문제는 <strong>모델이 내부 지식을 기반으로 답변하도록 장려하고 싶지만 이 내부 지식에 무엇이 포함되어 있는지 알 수 없다</strong>는 것이다.  </p>\n<p>지도학습 에서는 모델에 질문과 정답을 제시하고, 제공된 답변을 복제하도록 모델을 학습시킨다.<br>\n두 가지 경우가 존재한다.</p>\n<ol>\n<li>모델이 답을 <strong>알고 있다.:</strong> 이 경우 사전학습은 답변을 질문과 연결하도록 하고, 향후 유사한 질문에 답변하기 위해 유사한 단계를 수행하도록 학습한다.</li>\n<li>모델이 답을 <strong>모른다.:</strong> 이 경우 지도학습은 모델이 어떻게든 답변을 질문과 연관시키도록 한다. 모델이 특정 질문-답변 쌍을 기억하도록 모델을 학습한다. 그러나 이는 효과적이지 않다. <strong>우리의 목표는 모델이 지침 학습 데이터의 질문뿐만 아니라 모든 질문에 답하는 방법을 일반화하고 학습하는것이기 때문이다.</strong> 그러나 이러한 경우 일반화를 위한 모델학습에 성공한다면 본질적으로 모델이 내용을 구성하도록 가르치는 것이다. 모델이 “거짓말”을 하도록 적극적으로 장려하기때문에 이것은 나쁘다고 볼수있다.</li>\n</ol>\n<p>모델이 무엇을 알고있는지 모르기 때문에 지도학습에서 2번을 피하기 힘들다.\n<strong>진실한 답변을 생성하기 위한 모델을 추진하기 위해 RL을 사용</strong>해야 한다. 지도학습 설정과 달리 RL은 모델이 거짓말을 하도록 적극적으로 장려하지 않기 때문이다.\n<strong>모델이 처음 일부 답변을 정확하게 추측하고 실수로 만들어내도 장기적으로는 나쁜 점수를 받게 되기 때문이다. RL을 통해 잘못된 가능성이 있는 답변에 대해 내부 지식에 의존하거나 답을 포기하도록 학습시키는 것으로 볼 수 있다.</strong></p>\n<p><strong>절제하는 방법</strong>\n우리는 모델이 답을 모르는 경우에는 “모르겠어요”라는 대답으로 응답하기를 원한다. 이것은 쉽지 않은 일이다.  </p>\n<p>모델이 무엇을 알고 있는지 모르기 때문에 지도학습에서는 이를 수행하기가 어렵다. 우리는 특정 종류의 질문에 대답하지 않고 대신 “모르겠어요” 라고 대답하도록 강요할 수도 있다. 그러나 이는 답을 알 수 없을때 포기가 담긴 의도된 행동이 아니다.  </p>\n<p>이를 해결하는 한 가지 방법은 지도학습으로 시작하여 경우에 따라 “모르겠어요”라는 답변을 생성한 다음 RL을 사용하여 프로세스를 계속 하는것이다. 지도학습 및 RL 연구사례에서 모두 모델이 “모르겠어요”를 과도하게 생성하는 방법을 학습하는것 이라는 우려도 있다.  </p>\n<p>가능한 방법 중 하나는 정답에 매우 높은 점수를 할당하고 답변포기에 중간 이하 점수를, 오답에 강한 부정적인 점수를 할당하는 보상을 맞춤화하는 것이다. 그러나 이것을 바로잡기는 쉽지 않다.  </p>\n<h2 id=\"hfhuman-feedback없는-rl을-향하여\" style=\"position:relative;\"><a href=\"#hfhuman-feedback%EC%97%86%EB%8A%94-rl%EC%9D%84-%ED%96%A5%ED%95%98%EC%97%AC\" aria-label=\"hfhuman feedback없는 rl을 향하여 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HF(Human Feedback)없는 RL을 향하여</h2>\n<p>오랫동안 RL을 사용한 언어생성 task는 대부분에 사용자에게 비실용적이였다.\n<strong>신뢰할 수 있는 메트릭이 부족하고</strong> RL 학습에는 모든 샘플에 대한 <strong>인간의 피드백</strong>이 필요하기 때문이다.\n이 과정은 <strong>비용이 많이들고 매우 느리다.</strong> 특히 학습을 하기 위해 수천~수만 심지어 수십만개의 샘플을 확인해야 하는 경우 더욱 그렇다.  </p>\n<p>그러나 이제 RL 학습이 실용적이 되었다. 먼저 사전학습된 LLM은 <strong>적은 수의 샘플을 통해 학습할 수 있는것으로 보여진다. 그러나 더 중요한 것은 RL loop에서 인간을 제거하는 길을 열었다는 것이다.</strong>  </p>\n<p>이는 텍스트 기반 작업의 경우 지도학습 패러다임이 매우 효과적이며 대규모 모델이 일부 작업을 매우 잘 수행하는 방법을 학습할 수 있다는 관찰에 의존한다.  </p>\n<p>그러한 작업 중 하나는 두 개의 텍스트를 고려하고 “두 텍스트가 동일한 것을 의미하나요?” 라고 묻는것이고, 다른 하나는 “텍스트B에 나타나지 않는 사실이 텍스트A에 있나요?”이다.  </p>\n<p>이를 통해 RL에서 사용할 수 있는 효과적인 자동 스코어링 매커니즘으로 사용할 수 있다. 인간이 제공한 지침-응답 쌍에 대해 학습하고 모델이 자체 응답을 생성하고 비교하도록 한다. 지도방식으로 학습된 전용 텍스트 비교모델을 사용하여 인간이 제공한 응답에 대한 응답을 생성하는것이다.  </p>\n<p><strong>아직 개선되어야 할것들이 많지만 궁극적으로는 HF가 없는 RL으로 가야한다고 생각한다.</strong></p>","frontmatter":{"title":"Reinforcement Learning for Language Model","date":"December 16, 2023"}}},"pageContext":{"slug":"/NLP/rlforlm/","previous":{"fields":{"slug":"/NLP/chatgpt_beyond/"},"frontmatter":{"title":"ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning","category":"NLP","draft":false}},"next":null}},"staticQueryHashes":["3128451518","96099027"]}