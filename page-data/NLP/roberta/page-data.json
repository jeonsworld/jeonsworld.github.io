{"componentChunkName":"component---src-templates-blog-post-js","path":"/NLP/roberta/","result":{"data":{"site":{"siteMetadata":{"title":"Deep Learner","author":"[Jeonsworld]","siteUrl":"https:jeonsworld.github.io","comment":{"disqusShortName":"","utterances":"jeonsworld/blog-comment"},"sponsor":{"buyMeACoffeeId":""}}},"markdownRemark":{"id":"8e8d996b-27ff-5c84-a801-216bdaed8e82","excerpt":"RoBERTa: A Robustly Optimized BERT Pretraining Approach Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov https://arxiv.org/abs/1907.11692 Short Review: 개선된점만 정리 vs. BERT large-scale text copora…","html":"<blockquote>\n<p><strong>RoBERTa: A Robustly Optimized BERT Pretraining Approach</strong><br>\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov<br>\n<a href=\"https://arxiv.org/abs/1907.11692\">https://arxiv.org/abs/1907.11692</a></p>\n</blockquote>\n<h1 id=\"short-review-개선된점만-정리\" style=\"position:relative;\"><a href=\"#short-review-%EA%B0%9C%EC%84%A0%EB%90%9C%EC%A0%90%EB%A7%8C-%EC%A0%95%EB%A6%AC\" aria-label=\"short review 개선된점만 정리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Short Review: 개선된점만 정리</h1>\n<h2 id=\"vs-bert\" style=\"position:relative;\"><a href=\"#vs-bert\" aria-label=\"vs bert permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>vs. BERT</h2>\n<ul>\n<li>\n<p>large-scale text copora dataset(160GB)</p>\n<ol>\n<li>BookCorpus + Wikipedia (16GB)</li>\n<li>\n<p>CC-News (76GB)</p>\n<ul>\n<li>CommonCrawl News Dataset의 영어부분에서 수집한 CC-News Dataset.</li>\n<li>2016년 9월 ~ 2019년 2월 사이에 크롤링된 6300만개의 영문 뉴스 기사 포함.</li>\n</ul>\n</li>\n<li>\n<p>OpenWebText (38GB)</p>\n<ul>\n<li>OpenGPT2에서 설명한 WebText Corpus의 open-source recreation.</li>\n<li>Reddit에서 공유되는 URL에서 추출한 Web Contents</li>\n</ul>\n</li>\n<li>\n<p>Stories (31GB)</p>\n<ul>\n<li>CommonCrawl Data의 하위 집합을 포함.</li>\n<li>이야기와 같은 스타일의 Winograd schema와 일치하도록 filtering</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>Dynamic Masking</p>\n<ul>\n<li>기존의 BERT는 data preprocessing과정에서 masking을 한번 수행.</li>\n<li>본 논문에서는 data를 10개 복제하여 각 sequencerk 40 epoch에 걸쳐 10가지 방법으로 masking되도록 처리.</li>\n<li>즉, 훈련 중 동일한 mask는 4번만 보게 됨. 이 전략은 큰 데이터셋을 pre-train할때 중요함.\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 387px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.972972972972975%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABzklEQVQoz11Si27aUAzl/79km7RpbN2DQIEkBQoFkrbAeCa8IQmQBwgqFTizzcK6WbJsn+t7fG3fxPl8Bgtb1tfjEW+x2B4Jj+V0OpOe/sNOkpcol8vIZm+h6zo0TUe9VoOuacjn84RpyOVyhGuoVavip1IpFAoFlO7vJVdR0oLXyOcCiV+tFr5+SeLnj+/4+OkzbNtGqXiHd+8/UKEsMpkM0qSL+RTpdAbJZFIKafodYTPc3HyT4p1OR16acJZLdNpt9Pt9WJaF1WqF3W5H1gMXc1wX2+0WQRAK3uv1MB6P4fsBwjDEZDIRsiAILi1Pp1M8Pz2h2WyiTQdhGGF/OAgJF3FdT3y+zMp5o9EIAfnbKKKOLME4FkKuJknDIUzTRKVSgWHUYdQNwU3zESN5kU+vXmO/3wvxer0WjYiUC/I5LyYRhT4ajWe43koOLkkhPM8jwgbmiyUN+3Td5nw2Q0gksfibDRzHvcZEGKDb7dKsvOs3iYXbOby8/IMNBgMsHecaLxcLwqy/hAa1yZtUFAU52hZvmZWHr1LcosUMaRwxrqoqisWSEDNefXiASt/KtoeXlnk73D8PlX1umTfNluMNtRTHMfY2T+7+yWPC36xd3M2Oo0NAAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table1\"\n        title=\"table1\"\n        src=\"/static/e6b5a6ba6dff59b0a4fa65deb79413b2/691c3/ro_table1.png\"\n        srcset=\"/static/e6b5a6ba6dff59b0a4fa65deb79413b2/12f09/ro_table1.png 148w,\n/static/e6b5a6ba6dff59b0a4fa65deb79413b2/e4a3f/ro_table1.png 295w,\n/static/e6b5a6ba6dff59b0a4fa65deb79413b2/691c3/ro_table1.png 387w\"\n        sizes=\"(max-width: 387px) 100vw, 387px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n표 1: Dynamic Masking이 Static Masking과 비교하여 비슷하거나 약간 높다. 추가적인 이점을 감안하여 나머지 실험에서 Dynamic Masking 사용.</li>\n</ul>\n</li>\n<li>\n<p>Model Input Format and Next Sentence Prediction</p>\n<ul>\n<li>BERT에서 pre-train단계에 두 개의 연결된 segment를 관찰하여 동일한 문서인지 아닌지에 대해 NSP loss를 통해 train된다.</li>\n<li>Devlin et al.(2019)는 NSP를 제거하면 QNLI, MNLI, 및 SQuAD1.1에서 성능이 크게 저하된다고 하였다.</li>\n<li>그러나 최근연구(Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019)에서는 NSP를 손실의 필요성에 대해 의문을 제기함.</li>\n<li>\n<p>이러한 불일치를 더 잘 이해하기 위해 몇 가지 대체 training format을 비교한다.</p>\n<ul>\n<li><strong>SEGMENT-PAIR+NSP:</strong> 기존의 BERT에서 사용된 원래 input format을 따르며 NSP를 사용함. 각 입력에는 여러개의 문장이 포함되며 한 쌍의 segment가 있지만 512미만 길이의 token으로 구성.</li>\n<li><strong>SENTENCE-PAIR+NSP:</strong> 각 입력에는 document의 인접부분 또는 다른 document의 sentence 쌍으로 구성됨. 이 입력은 512 token보다 훨씬 짧기 때문에 총 갯수가 <strong>SEGMENT-PAIR+NSP</strong>와 유사하게 유지되도록 batch size를 늘린다. NSP 또한 사용.</li>\n<li><strong>FULL-SENTENCES:</strong> 각 입력은 하나 이상의 document에서 연속적으로 sampling된 전체 sentence로 채워지므로 총 길이는 최대 512이다. document가 끝나면 다음 document에서 sentence를 sampling하며 document사이에 별도의 separator token을 추가함. NSP는 사용하지 않음.</li>\n<li><strong>DOC-SENTENCES:</strong> 입력이 document 경계를 넘을 수 없다는 점을 제외하고 <strong>FULL-SENTENCES</strong>와 유사하게 구성된다. document의 끝 부분에 sampling된 입력은 512개의 token보다 짧을 수 있으므로 이러한 경우 전체 batch와 비슷한 수의 전체 sentence를 얻기 위해 동적으로 batch size를 늘린다. NSP는 사용하지 않음.</li>\n</ul>\n</li>\n<li>\n<p><strong>Results</strong></p>\n<ul>\n<li><strong>SEGMENT-PAIR+NSP</strong>와 <strong>SENTENCE-PAIR+NSP</strong>를 먼저 비교해보면 두 format모두 NSP를 유지하지만 후자는 single sentence를 사용한다.</li>\n<li>single sentence를 사용하는 것이 downstream task에 대한 성능을 저해한다는 것을 알아냄.</li>\n<li>이유는 model이 long-range dependency를 capture할 수 없기 때문.</li>\n<li>NSP loss를 사용하지 않는 <strong>DOC-SENTENCES</strong>과 비교하면 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>B</mi><mi>E</mi><mi>R</mi><mi>T</mi></mrow><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{BERT}_{BASE}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault mtight\">A</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">E</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>보다 성능이 좋고 NSP loss를 제거하는 것이 downstream task의 성능을 약간 향상시키는것으로 나타났다.</li>\n<li>single document(DOC-SENTENCES)에서 오는 sequence가 multiple document(FULL-SENTENES)의 sequence를 packing하는것보다 약간 더 잘 수행되는것으로 확인되었다.</li>\n<li>그러나 <strong>DOC-SENTENCES</strong>는 variable batch size를 사용해야하기 때문에 실험을 쉽게 진행할 수 있도록 나머지 실험들에 대해서는 <strong>FULL-SENTENCES</strong>를 사용.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 556px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.37837837837838%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB60lEQVQozzVS2W7CQAzM/39UqQSC0rcSEsgdckMCCYSjnFOPK1ay1uv12jPjNWzbxmw2Q5ZlyPNcjf5C4qZpwnEcLJdLBEEA5przOXzfh+95mIsfRTEsy8JiscD5fIYxmUww+PxEWZaoqkqNRUejIT4GA3zJPXNsefA9nWI4HGkja25iPB7Dk+IT2afTb5xOJxjr9Ro013EVRbJaoSgKbDYbJEmKJE0VcdftwfV8PnV/vV54PB66097LiKNIIBN6pAWLotSL6/WqtB1nie1up7Fj38OSGAv1hwM8z9d4EPgKQAtSI2pDnejTCqGfZanGqQ1jb23DMJTHtUq0EjZN0yCOY2Vzu91gFJJYVWvp5mkyrShy7eh5LkJBngnttZzbtsXlclGK9/tdh0Cf2pGRIiy0c45AxCUFiny73fXBG3Xbdprc9wed8FOKHPYdfJGIBUPZOQctyNFz7NSQsFNB02y3Smn286N3pLaVWC6UHddVmpTEkUHWdQ1XYsxRyq4gsCxbHjSCoMfl9xcPQcdVyrTZiNS4uFOSf/8k2q3UT5IE+/3/LzA8qc5/tYojHUAQRqgFAVfT1FhKw+PxqNS6rtUcNu3anVImKn5yItWC/CqEzNHTp3FqnCjRUTPS4Zk7z0REn7mU6P2GDP4AfJAqBY8sWAwAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table2\"\n        title=\"table2\"\n        src=\"/static/95e2d72c07ca7dcbd7b6c3a140214497/96638/ro_table2.png\"\n        srcset=\"/static/95e2d72c07ca7dcbd7b6c3a140214497/12f09/ro_table2.png 148w,\n/static/95e2d72c07ca7dcbd7b6c3a140214497/e4a3f/ro_table2.png 295w,\n/static/95e2d72c07ca7dcbd7b6c3a140214497/96638/ro_table2.png 556w\"\n        sizes=\"(max-width: 556px) 100vw, 556px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n표 2: BookCorpus + WikiPedia에 Pre-train된 base model에 대한 결과. </p>\n</li>\n</ul>\n</li>\n<li>\n<p>Training with large batches</p>\n<ul>\n<li>기계번역(Neural Machine Translation)의 과거 연구는 매우 큰 mini batch를 사용한 학습은 학습속도가 적절하게 증가할 때 최적화 속도와 최종 작업 성능을 향상시킬 수 있음을 보여주었다. (Ott et al., 2018)</li>\n<li>최근 연구결과에 따르면 BERT는 대규모 batch training에도 적합하다. (You et al., 2019)</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mrow><mi>B</mi><mi>E</mi><mi>R</mi><mi>T</mi></mrow><mrow><mi>B</mi><mi>A</mi><mi>S</mi><mi>E</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">{BERT}_{BASE}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathdefault mtight\">A</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">E</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>는 1M step에 대해 batch size 256 sequence로 train하였다.</li>\n<li>gradient accumulation을 통한 computation cost측면에서 보면, 2K sequence의 batch size로 125K step, 8K sequence의 batch size로 31K step과 동일하다.</li>\n<li>표 3을 통해 large batch를 통한 training이 masked language modeling뿐만 아니라 end-task accuracy도 향상시키는 것을 확인할 수 있다.</li>\n<li>실험에서는 8K sequence batch를 사용한다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 403px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.13513513513513%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABcklEQVQozz2Qi26CQBBF/f9f0hoTK6ilUEGeAj6A5a0i0Fhqcju7xk4yYWZ353DnjkBRn88oigJJktC3RFVViOMYaZaBMSaSn/H7LM+Ri8zEDL/jPWMprpcLRhwYRxHm8znq+oz1aonZbIbrtYFtWUhoIEsTvE0mYFmOlSxhPB5DXq6gqhpOpyOm0yneab65tU9gSX/StC+hzrZMrNcfBK8RBgEiUhrTkCTJBE+xDwNsDQO27cD3fTBSbVk2XNdF0zRPYHM9Q1EUdP03ioxhoxuiDnxPrM7hBkFYmuLxeMA0dByOEYZhQN930DcbxAkTvQAOww92O5+XwhuLVv2lwaoscGtb9F0LSZZRkiU8tgQMwj1eoaqfOEWxqAWwKkssFhK6rsPOdaCQNzfyw3Vs7PcHsiKHaZpI04wU9dA1FYZpoW073GhNjXqPBN3v9yeQsUSoCsOQlHrCD147jkP9DkHgUwb/6dI5f/PqPc8Tb0sS9gee1/+XfDB3dQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"table3\"\n        title=\"table3\"\n        src=\"/static/e78f8f6cbe5479011dc92acb167d23e9/045fd/ro_table3.png\"\n        srcset=\"/static/e78f8f6cbe5479011dc92acb167d23e9/12f09/ro_table3.png 148w,\n/static/e78f8f6cbe5479011dc92acb167d23e9/e4a3f/ro_table3.png 295w,\n/static/e78f8f6cbe5479011dc92acb167d23e9/045fd/ro_table3.png 403w\"\n        sizes=\"(max-width: 403px) 100vw, 403px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n    </span>\n표 3: 다양한 batch size로 train된 bert_base에 대한 결과. 모두 동일한 계산비용.</p>\n</li>\n<li>\n<p>Text Encoding(약간의 성능 향상이 있을거라는 가설, 실험은 진행하지 않음)</p>\n<ul>\n<li>OpenGPT2의 BPE는 유니코드 문자 대신 byte를 기본 하위 단위로 사용하는 BPE를 사용함.</li>\n<li>이것을 사용하면 “unknown” token 없이도 input text를 encoding할 수 있는 적당한 크기의 하위 단어어휘를 train할 수 있음.</li>\n<li>본 논문에서는 이를 사용하였으며 encoding에 대한 실험은 future work로 남겨둠.</li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"[논문리뷰] RoBERTa: A Robustly Optimized BERT Pretraining Approach","date":"July 17, 2019"}}},"pageContext":{"slug":"/NLP/roberta/","previous":{"fields":{"slug":"/Graph_Neural_Network/segtree_tf/"},"frontmatter":{"title":"[논문리뷰] SegTree Transformer: Iterative Refinement of Hirarchical Features","category":"Graph_Neural_Network","draft":false}},"next":{"fields":{"slug":"/NLP/spanbert/"},"frontmatter":{"title":"[논문리뷰] SpanBERT: Improving Pre-training by Representing and Predicting Spans","category":"NLP","draft":false}}}},"staticQueryHashes":["3128451518","96099027"]}